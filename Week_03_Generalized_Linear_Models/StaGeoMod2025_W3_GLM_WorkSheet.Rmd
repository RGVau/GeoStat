---
title: "GLMs Lab: Count & Proportion Data"
subtitle: "A full workflow with interpretation, diagnostics, and model simplification"
author: "René Gier Vemmelund"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    self_contained: yes
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error = TRUE)
set.seed(123)
library(tidyverse)
library(MASS)        # glm.nb, stepAIC
```

## Learning goals

-   Choose and fit appropriate GLMs for **count** (Poisson / Negative
    Binomial) and **proportion** (Binomial / quasi-Binomial) data.\
-   Interpret coefficients: **rate ratios** (counts) and **odds ratios**
    (proportions).\
-   Assess fit via **deviance, dispersion**, residual diagnostics, and
    **model simplification** (LRT, AIC).\
-   Make predictions and visualise with **ggplot2**.

::: callout-note
**What you’ll submit**:

An HTML version of this exercise by the end of the practial session
:::

## Lab Timeline (2 hours)

| Time | Activity |
|-----------------|-------------------------------------------------------|
| 0:00–0:15 | **Introduction to GLMs** – recap linear model limits, why we need GLMs. |
| 0:15–0:35 | **Exploratory Data Analysis (EDA)** – plotting counts and proportions with ggplot2. |
| 0:35–1:05 | **Count data example (Negative Binomial GLM)**: simulate / load data, fit Poisson, check overdispersion, refit with `glm.nb()`, interpret coefficients, plot fitted vs observed. |
| 1:05–1:25 | **Proportional data example (Binomial GLM)**: fit model using `glm(..., family=binomial)`, interpret log-odds & odds ratios, check residuals, plot fitted probabilities with CIs. |
| 1:25–1:40 | **Model simplification & comparison**: use `anova()`, `drop1()`, and AIC for model selection. |
| 1:40–1:50 | **Diagnostics**: residual vs fitted plots, dispersion checks, discussion of overdispersion and quasi families. |
| 1:50–2:00 | **Wrap-up**: Key takeaways, when to use Poisson, Negative Binomial, Binomial/Quasi-Binomial, interpretation of multiplicative vs additive effects. |

## Part A — Count data (≈ 60 min)

### A1. Simulate “roadkills”-style counts

::: {.alert .alert-info}
**Task.**

Create a simulation that generates synthetic bird count data based on
three environmental predictors:

1)  `OPEN.L`: Percentage of open land (0-100%)
2)  `D.PAR`K\`: Distance to nearest park (0-5000 meters)
3)  `L.WAT.C`: Length of nearby watercourses (0-5 km)

**Your Mission**

***Step 1*****: Set up the simulation parameters**

Generate data for 600 observation sites Create realistic ranges for each
predictor variable using appropriate random distributions

***Step 2*****: Define the ecological relationships**

The true relationship follows this pattern:

1)  More open land → fewer birds (coefficient: -0.010)
2)  Greater distance to parks → fewer birds (coefficient: -0.00012)
3)  More watercourse length → more birds (coefficient: +0.18)
4)  Baseline log-abundance: 1.2

***Step 3*****: Generate realistic count data**

Use the linear predictor to calculate expected abundance Add ecological
realism by incorporating overdispersion ($\theta = 4$) Generate final
bird counts using an appropriate count distribution

***Step 4*****: Organize and explore your data**

Combine all variables into a clean data frame Examine the structure and
summary of your simulated dataset
:::

```{r}
# Simulate predictors
# Step 1: Simulation setup
set.seed(123)  # For reproducible results
n <- 600       # Number of sites - 600

# Step 2: Generate predictor variables
OPEN.L <- runif(n, 0,100)     # Hint: uniform distribution, 0 to 100
D.PARK <- runif(n, 0,5000)    # Hint: uniform distribution, 0 to 5000
L.WAT.C <- runif(n, 0, 5)    # Hint: uniform distribution, 0 to 5

# Step 3: Create the ecological model
eta <- 1.2-0.010*OPEN.L-0.00012*D.PARK+L.WAT.C*0.18        # Linear predictor combining all effects
                  # Beta_OPEN.L = 0.010
                  # Beta_D.PARK = 0.00012
                  # Beta_L.WAT.C = 0.18
mu <- exp(eta)         # Transform eta to expected count scale
theta <- 4      # Overdispersion parameter (4)

# Step 4: Generate observed counts
TOT.N <- rnbinom(n, mu = mu, size = theta)      # Hint: use a random negative binomial distribution

# Step 5: Create final dataset merging TOT.N, OPEN.L, D.PARK, & L.WAT.C
datC <- tibble(TOT.N, OPEN.L, D.PARK, L.WAT.C)
datC
```

### A2. Poisson GLM, dispersion check, quasi-Poisson

::: {.alert .alert-info}
**Task.**

Fit a Poisson GLM as a baseline; compute dispersion. If dispersion \>\>
1, fit a quasi-Poisson to obtain robust SEs (note: no AIC for
quasi-families).

**Your Mission**

***Step 1*****: Fit a baseline Poisson GLM**

Use your simulated bird count data (TOT.N) with all three environmental
predictors to establish a starting model.

***Step 2*****: Check for overdispersion**

Calculate the dispersion parameter by comparing residual deviance to
degrees of freedom. A value \>\> 1 indicates overdispersion.

***Step 3*****:\* Apply quasi-Poisson correction if needed**

If overdispersion is detected, fit a quasi-Poisson model to obtain
robust standard errors that account for extra variability.

***Step 4*****: Compare model results**

Examine how overdispersion correction affects coefficient estimates,
standard errors, and statistical significance
:::

```{r}
# Fit Poisson GLM
m_pois <- glm(TOT.N ~ OPEN.L + D.PARK + L.WAT.C, 
              family = poisson(), 
              data = datC)

# Examine the results
summary(m_pois)

# Calculate dispersion parameter
disp_pois <- m_pois$deviance / m_pois$df.residual
print(paste("Dispersion parameter:", round(disp_pois, 2)))

# Fit quasi-Poisson GLM
m_qp <- glm(TOT.N ~ OPEN.L + D.PARK + L.WAT.C, 
            family = quasipoisson(), 
            data = datC)

# Compare results
summary(m_qp)

# Extract coefficients and SEs
pois_coef <- summary(m_pois)$coefficients[, 1:2]
qp_coef <- summary(m_qp)$coefficients[, 1:2]

# Create comparison
comparison <- data.frame(
  Predictor = rownames(pois_coef),
  Poisson_Coef = pois_coef[, 1],
  Poisson_SE = pois_coef[, 2],
  QuasiPois_Coef = qp_coef[, 1],
  QuasiPois_SE = qp_coef[, 2],
  SE_Inflation = qp_coef[, 2] / pois_coef[, 2]
)

print(comparison)
```

::: {.alert .alert-success}
**Question**

1)  **Model Selection**: When would you choose quasi-Poisson over
    Poisson?

I would choose a quasi-Poisson over a Poisson if the data is
overdispersed (\>\>1). However the Residual deviance does not change, so in this instance the quasi is not better.

2)  **Limitations**: Why can't you calculate AIC for quasi-families?

AIC requires likelihood function which quasi-models do not have. Quasi uses restricted maximum likelihood (they specify only mean and variance relationship).


3)  **Alternatives**: What other approaches could handle overdispersion?
    (*Hint*: negative binomial)
    
I would fit a negative binomial model, as this has a dispersion parameter,θ, that estimates how much the variance exceeds the mean, models variance as μ + μ²/θ.

*Biological Interpretation:*

1)  Which environmental factor has the strongest effect on bird counts?

It depends on the units of the predictors. But when scaling the predictors, L.WAT.C has the strongest effect.

2)  How do you interpret the coefficient for `D.PARK`?
An increase in 1 m to a park (exp(-0.0001286337)=0.999) have 0.01% less bird counts will be counted.

:::

### A3. Negative Binomial GLM & comparison

::: {.alert .alert-info}
**Task.**

Fit a Negative Binomial GLM and compare to Poisson using AIC. Prefer NB
if it provides a better likelihood-penalised fit.

**Your Mission**

***Step 1*****: Fit a Negative Binomial GLM**

Use the `glm.nb()` function to fit a negative binomial model that
explicitly accounts for overdispersion by estimating a dispersion
parameter.

***Step 2*****: Examine the model summary**

Review the coefficient estimates, standard errors, and the estimated
theta (dispersion) parameter to understand how NB differs from Poisson.

***Step 3*****: Compare models using AIC**

Calculate AIC values for both Poisson and Negative Binomial models to
determine which provides better model fit penalized for complexity.

***Step 4*****: Make an informed model choice**

Select the preferred model based on AIC comparison and biological
interpretability of results.
:::

```{r}
library(MASS)  # Required for glm.nb()

m_nb <- glm.nb(TOT.N ~ OPEN.L + D.PARK + L.WAT.C,
              data = datC)
summary(m_nb)

# Compare models using AIC
AIC(m_pois, m_nb)
```

::: {.alert .alert-success}
**Questions**

1.  **Theta parameter**: What is the estimated theta ($\theta$) value in
    your negative binomial model? How does this compare to the true
    value ($\theta = 4$) used in your simulation?

*Theta equaled 4.018, so a little higher than used in the simulation.*

2.  **Standard errors**: Compare the standard errors between the Poisson
    and Negative Binomial models. Which model has larger standard errors
    and why?

*The Residual deviance reduced from 1039.0 to 677.75. The Std. errors for the coefficients were larger for the Poisson, as the over-dispersion increased the uncertainty of the coefficients.*

3.  **AIC comparison**: Which model has the lower AIC value? What does
    this tell you about model fit?

*The negative binomial model. It tells me that the model fit was better.*

4.  **Coefficient interpretation**: Are the coefficient signs and
    magnitudes similar between Poisson and NB models? What does this
    suggest about the robustness of your ecological conclusions?

*Signs and magnitudes are very similar across Poisson, quasi-Poisson, and NB. This suggests the estimated ecological relationships are robust to choice of model and differences mostly affect uncertainty (SEs, significance) and model fit.*

5.  **Model choice**: Based on your AIC comparison, which model would
    you choose for inference and why?

*Based on AIC and the dispersion check, I would choose the nb for inference — it models has better AIC, and provides valid SEs and likelihood-based comparisons: something that quasi-poisson does not provide.*

6.  **Practical implications**: How might using the wrong model (Poisson
    vs. NB) affect your conclusions about which environmental factors
    significantly influence bird abundance?

*In this example affect is small, as the coefficients were similar across models. But the overdispersion could in other examples introduce uncertainty for when predictors have a significant effect.*
:::

### A4. Model simplification (drop-one, stepAIC)

::: {.alert .alert-info}
**Task.**

Simplify the NB model: inspect drop-one Chi-square tests, then apply
stepAIC to seek a parsimonious model.

**Your Mission**

***Step 1: Conduct drop-one tests***

Use `drop1()` with Chi-square tests to evaluate the individual
contribution of each predictor when removed from the full model.

***Step 2: Apply automated model selection***

Use `stepAIC()` to systematically search for the most parsimonious model
by comparing AIC values across different predictor combinations.

***Step 3: Examine the final simplified model***

Review the summary of the selected model to understand which predictors
were retained and their statistical significance.

***Step 4: Interpret coefficients on the natural scale***

Transform log-scale coefficients to rate ratios (multiplicative effects)
for easier ecological interpretation.
:::

```{r}
# Step 1: Perform drop-one Chi-square tests
drop1(m_nb, test = "Chisq")    # Test individual predictor contributions

# Step 2: Apply stepwise AIC model selection  

m_nb_step <- stepAIC(m_nb, trace = TRUE)    # Find most parsimonious model
                                          # trace = FALSE suppresses output

# Step 3: Examine the simplified model
summary(m_nb_step)    # Review final model structure and significance

# Step 4: Calculate rate ratios (multiplicative effects)
exp(coef(m_nb_step))    # Transform coefficients to natural scale
                  # exp() converts log-effects to multiplicative effects
```

::: {.alert .alert-success}
**Questions**

## Interpretation Questions

1.  **Drop-one results**: Which predictor(s) show the highest Chi-square
    values in the drop-one test? What does this indicate about their
    importance?

*Open.L and L.WAT.C. This indicates that they contribute the most to model deviance.*

2.  **Model selection outcome**: Did stepAIC retain all three
    predictors, or was the model simplified? Which variables (if any)
    were dropped?

*No, predictors were dropped.*

3.  **AIC improvement**: Compare the AIC of the final stepped model with
    the original full model. Is there evidence that simplification
    improved the model?

*The stepped model returned the same predictors, so simplification did not improve the model.*

4.  **Rate ratio interpretation**: Looking at the `exp(coef())` output,
    how do you interpret the multiplicative effects? For example, what
    happens to bird abundance for every 1% increase in open land?

*The multiplicative effect of OPEN.L is 0.9903. So for every 1% increase in open land, there is a 0.97% decrease in bird counts.*

5.  **Ecological significance**: Which environmental factor has the
    strongest multiplicative effect on bird abundance? How would you
    translate this into management recommendations?

*L.WAT.C has the strongest effect (≈ +20% per km). Management implication: restoring nearby watercouses could increase bird abundance; small changes in open land % or small park distances have much smaller per-unit impacts. But if you hated birds you would decrease nearby watercourse length.*

6.  **Statistical vs. biological significance**: Are all retained
    predictors both statistically significant (p \< 0.05) and
    ecologically meaningful? How do you distinguish between the two?

*All predictors are statistically significant (p < 0.001). To judge ecological significance I would assess if change in the predictors would change the amount of habitat for birds, and I would suggest that all predictors change habitat suitability for many bird species.*
:::

### A5. Predictions (response scale) & CI plot

::: {.alert .alert-info}
**Task.**

Produce fitted means and 95% confidence bands on the response scale for
a key covariate (hold others at medians); plot observed counts + fitted
curve.

**Your Mission**

***Step 1: Create a prediction grid***

Generate a sequence of values for your focal predictor (`OPEN.L`) while
holding other predictors at their median values to isolate the effect of
interest.

***Step 2: Generate model predictions***

Use the simplified model to predict bird counts across the range of open
land percentages, including standard errors for uncertainty
quantification.

***Step 3: Transform predictions to response scale***

Convert log-scale predictions and confidence intervals back to the count
scale for meaningful interpretation.

***Step 4: Create a publication-ready plot***

Combine observed data points with model predictions and confidence bands
to visualize the relationship.
:::

```{r}
# Step 1: Create prediction grid for focal variable
gridC <- tibble(
  OPEN.L = seq(min(datC$OPEN.L), max(datC$OPEN.L), length.out = 300),  # Sequence of 300 elements from min to max OPEN.L
  D.PARK = median(datC$D.PARK),                                        # Hold D.PARK at median value
  L.WAT.C = median(datC$L.WAT.C, na.rm = TRUE)                       # Hold L.WAT.C at median value
)

# Step 2: Generate predictions with standard errors
pl <- predict(m_nb_step, newdata = gridC, type = "link", se.fit = TRUE)        # Get link-scale predictions + SEs
                                                                      # type = "link" gives log-scale
                                                                      # se.fit = TRUE includes SEs

# Step 3: Transform to response scale with confidence intervals
gridC <- gridC |>
  mutate(
    mu = exp(pl$fit),                           # Transform fitted values to count scale
    lo = exp(pl$fit - 1.96*pl$se.fit),        # Lower 95% CI boundary
    hi = exp(pl$fit + 1.96*pl$se.fit)         # Upper 95% CI boundary
  )

# Step 4: Create the prediction plot
ggplot(datC, aes(y=TOT.N, x=OPEN.L)) +                                         # Plot original data
  geom_point(alpha = 0.8) +                                          # Add semi-transparent points
  geom_ribbon(data = gridC, aes(x = OPEN.L, ymin = lo, ymax = hi), 
              alpha = 0.8,
              inherit.aes = FALSE) +                    # Add confidence band
  geom_line(data = gridC, aes(x = OPEN.L, y = mu), 
            linewidth = 1, inherit.aes = FALSE) +                  # Add fitted line
  labs(title = "Plot: fitted counts vs % open land",
       x = "Percentage of open land",
       y = "Bird count") +                                                  # Add informative labels
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

1.  **Prediction grid setup**: Why do we hold D.PARK and L.WAT.C at
    their median values when creating predictions for OPEN.L? What would
    happen if we used different values?

*Holding other predictors at their medians isolates the effect of OPEN.L on expected counts. Using other values (e.g. 10th or 90th percentiles) would shift the predicted curve up/down (different baseline) and could change interpretations.*

2.  **Link vs. response scale**: Why do we first predict on the "link"
    scale and then transform using `exp()`, rather than predicting
    directly on the response scale?

*GLMs for counts use a log link: the model is linear on the log scale. We predict on the link scale because the model’s fit/SE assumptions are normal there. We then take the exp to transform to the response (count) scale. Predicting directly on the response scale would not give correct (symmetric) conf inds because the distribution is skewed.*

3.  **Confidence interval interpretation**: What do the confidence bands
    around the fitted line represent? How would you explain this to a
    non-statistician?

*The bands show where we expect the average bird count to fall for sites with that percent open land (holding other variables fixed at medians), with 95% confidence.*

4.  **Ecological pattern**: Describe the relationship between open land
    percentage and bird counts shown in your plot. Does this match your
    biological expectations?

*The fitted relationship shows bird counts decline as % open land increases, matching the simulated ecological expectation: birds use trees for rest, shelter and search for food, so more open -> less trees -> less birds.*

5.  **Model fit assessment**: Looking at how well the fitted line and
    confidence bands capture the observed data points, would you say
    this model provides a good fit? What might you look for to assess
    this?

*I would assess the fit to be good, as the prediction ribbon is quite narrow. Though there is a lot of scatter around the line, suggesting that there are other important predictors.*

6.  **Practical applications**: How could you use this plot to inform
    urban planning decisions about bird conservation?

*Use the plot to show decision-makers that increasing open land (e.g. removing shrubs/trees) tends to reduce bird counts.*
:::

### A6. Diagnostics: residuals & influence

::: {.alert .alert-info}
**Task.**

Inspect deviance/Pearson residuals vs fitted and influence diagnostics.
Look for structure (misspecification) and high-leverage points.

**Your Mission**

***Step 1: Extract diagnostic measures***

Create a comprehensive dataset containing fitted values, residuals, and
influence measures from your negative binomial model to assess model
assumptions and identify problematic observations.

***Step 2: Create residuals vs fitted plot***

Plot deviance residuals against fitted values to check for patterns that
might indicate model misspecification, non-linearity, or
heteroscedasticity.

***Step 3: Examine influence diagnostics***

Create a leverage vs Cook's distance plot to identify observations that
have high influence on model parameters or are potential outliers.

***Step 4: Interpret diagnostic patterns***

Assess whether the model adequately captures the data structure and
identify any observations requiring further investigation.
:::

```{r}
# Step 1: Extract all diagnostic measures into a tibble
diag_nb <- tibble(
  fitted  = fitted(m_nb_step),                           # Extract fitted values (predicted means)
  devres  = residuals(m_nb_step, type = "deviance"),         # Extract deviance residuals
  pearson = residuals(m_nb_step, type = "pearson"),         # Extract Pearson residuals  
  hat     = hatvalues(m_nb_step),                       # Extract leverage values (diagonal of hat matrix)
  cooks   = cooks.distance(m_nb_step)                   # Extract Cook's distance (influence measure)
)

# Step 2: Create deviance residuals vs fitted values plot
ggplot(diag_nb, aes(fitted, devres)) +                      # Plot fitted vs deviance residuals
  geom_hline(yintercept = 0, linetype = "dashed") +  # Add horizontal reference line at y=0 (dashed)
  geom_point(alpha = 0.5) +                       # Add semi-transparent points (0.5)
  geom_smooth(se = FALSE) +                         # Add smooth trend line without confidence bands
  labs(title = "Plot: deviance residuals vs fitted", 
       x = "Fitted values", 
       y = "Deviance residuals") +                               # Add descriptive labels
  theme_minimal()

# Step 3: Create leverage vs Cook's distance plot
ggplot(diag_nb, aes(hat, cooks)) +                      # Plot leverage vs Cook's distance
  geom_point(alpha = 0.6) +                       # Add semi-transparent points (0.6)
  labs(title = "Plot: leverage vs Cook's distance", 
       x = "Leverage", 
       y = "Cook's distance") +                               # Add descriptive labels
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

1.  **Residual types**: What's the difference between deviance and
    Pearson residuals, and why might we prefer deviance residuals for
    GLM diagnostics?

*Pearson residuals: They standardise each observation by the model variance function.

Deviance residuals: contribution of each observation to the model deviance (square-root of the per-observation deviance).

Why prefer deviance residuals for GLM diagnostics? Because they reflect model deviance (likelihood) directly.*

2.  **Residuals vs fitted interpretation**: In your deviance residuals plot, what pattern would indicate a well-fitting model? What patterns would suggest problems?

*Good fit: residuals scattered randomly around 0 with no systematic trend.

Problems: fan shape (variance changes with fitted), curvature (nonlinear relationship not captured).*

3.  **Reference line importance**: Why do we include a horizontal dashed
    line at y=0 in the residuals plot, and what does it represent?

*The dashed horizontal line at 0 represents no residual (observed = predicted). Points above the line have observed > predicted; below have observed < predicted. It aids visually.*

4.  **Leverage interpretation**: What does high leverage indicate about
    an observation, and why should we be concerned about high-leverage
    points?

*Leverage measures how unusual an observation’s predictor values are. High-leverage points can influence parameter estimates.*

5.  **Cook's distance threshold**: Cook's distance measures overall
    influence. What general threshold is often used to identify
    potentially problematic observations, and why?

*Common rules of thumb: points with Cooks D > 4/n (here 4/600 ≈ 0.0067) are worth checking. Another more conservative threshold is D > 1 for strong influence.*

6.  **Diagnostic integration**: How would you use both plots together to
    identify the most concerning observations? What would be the "worst
    case" combination of leverage and Cook's distance?

*Observations with both high leverage and high Cook’s distance (and/or large deviance residuals). The worst case is high leverage + large Cook’s D + large residual.*

7.  **Model adequacy assessment**: Based on your diagnostic plots, would
    you conclude that the negative binomial GLM adequately fits your
    simulated data? What evidence supports this conclusion?

*I would assess that the nb model fits the simulated data well. The part of the deviance residuals plot with many fitted values (fitted values < 5) show an even spread around the horizontal line. The Leverage x Cooks distance plot show that the single observations with high Cooks D has low leverage.*
:::
