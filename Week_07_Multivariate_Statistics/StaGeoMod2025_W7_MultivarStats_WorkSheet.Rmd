---
title: "Ordination and Classification of Vegetation Types"
subtitle: "Predicting Deciduous Broadleaf Forests using Climate Data"
author: "Student Name: René Gier Vemmelund"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    self_contained: yes
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(vegan)
library(tidyverse)

```

## Introduction

In this 90-minute practical, you will gain hands-on experience with multivariate ordination and classification techniques commonly used in ecology and biogeography. These methods help us understand how environmental variables structure biological communities and predict vegetation distributions.

### Learning Objectives:

- Apply Principal Component Analysis (PCA) to reduce dimensionality of environmental data
- Perform Principal Coordinates Analysis (PCoA) and Non-metric Multidimensional Scaling (NMDS)
- Use K-means clustering to classify vegetation types
- Interpret ordination plots and assess classification accuracy
- Predict the distribution of deciduous broadleaf forests based on climate variables

### Dataset Overview:

You have three datasets:

1. `BiomeData.csv` - Site locations (x, y coordinates) and biome classifications (500 sites per vegetation type)
2. `ClimateData.csv` - 19 bioclimatic variables plus elevation for each site
3. `WorldClimateData.csv` - Global climate data for prediction (not used in this practical)

The bioclimatic variables (bio.1 to bio.19) represent annual trends, seasonality, and extreme environmental factors derived from temperature and precipitation data.

### Bioclimatic Variables Reference Table

| Variable | Name | Description | Units |
|----------|------|-------------|-------|
| bio.1 | Annual Mean Temperature | Mean of all weekly mean temperatures | °C |
| bio.2 | Mean Diurnal Range | Mean of monthly (max temp - min temp) | °C |
| bio.3 | Isothermality | (bio.2/bio.7) × 100 | % |
| bio.4 | Temperature Seasonality | Standard deviation of weekly mean temperatures | °C × 100 |
| bio.5 | Max Temperature of Warmest Month | Highest temperature of any monthly maximum | °C |
| bio.6 | Min Temperature of Coldest Month | Lowest temperature of any monthly minimum | °C |
| bio.7 | Temperature Annual Range | bio.5 - bio.6 | °C |
| bio.8 | Mean Temperature of Wettest Quarter | Mean temperature during wettest quarter | °C |
| bio.9 | Mean Temperature of Driest Quarter | Mean temperature during driest quarter | °C |
| bio.10 | Mean Temperature of Warmest Quarter | Mean temperature during warmest quarter | °C |
| bio.11 | Mean Temperature of Coldest Quarter | Mean temperature during coldest quarter | °C |
| bio.12 | Annual Precipitation | Sum of monthly precipitation values | mm |
| bio.13 | Precipitation of Wettest Month | Precipitation of the wettest month | mm |
| bio.14 | Precipitation of Driest Month | Precipitation of the driest month | mm |
| bio.15 | Precipitation Seasonality | Coefficient of variation | % |
| bio.16 | Precipitation of Wettest Quarter | Total precipitation during wettest quarter | mm |
| bio.17 | Precipitation of Driest Quarter | Total precipitation during driest quarter | mm |
| bio.18 | Precipitation of Warmest Quarter | Total precipitation during warmest quarter | mm |
| bio.19 | Precipitation of Coldest Quarter | Total precipitation during coldest quarter | mm |

**Note:** Temperature variables are in degrees Celsius (°C), precipitation variables are in millimeters (mm). Quarter refers to a period of 3 consecutive months.

---

## Task 1: Principal Component Analysis (PCA)

### Background and Objectives

**What is PCA?**

Principal Component Analysis is an ordination technique that transforms correlated environmental variables into a smaller set of uncorrelated variables called principal components (PCs). Each PC is a linear combination of the original variables and captures a decreasing amount of variance in the dataset.

**Why use PCA?**

- Reduces dimensionality while retaining most variation in the data
- Identifies which environmental variables drive differences between sites
- Visualizes multidimensional data in 2D or 3D space
- Works best with linear relationships between variables

**What we will do:**

1. Load and prepare the climate and biome data
2. Standardize environmental variables
3. Perform PCA on the environmental data
4. Determine how many PC axes to retain
5. Interpret variable loadings
6. Visualize the ordination space colored by biome type
7. Add convex hulls

---

### Step 1.1: Load and Prepare Data

**What this step does:**
Import two CSV files containing biome classifications and climate data, merge them by their common ID column, extract the environmental variables, and check for missing values.

**Why we do this:**
We need to combine our ecological classification data with environmental predictors. Checking for missing values is essential because most statistical methods cannot handle missing data.

**Your task:** Write code to load the two CSV files, merge them, extract the 20 environmental variables (bio.1 through bio.19 plus Elev), and check for missing values.


```{r Load data}
# Load the data
biome_data <- read.csv("DATA/BiomeData.csv", row.names = 1)
clim_data <- read.csv("DATA/ClimateData.csv", row.names = 1)

# Merge datasets by ID
data_merged <- merge(biome_data, clim_data, by = "ID")

# Extract environmental variables (bio.1 to Elev)
env_vars <- names(clim_data)[-1]
env_data <- data_merged %>% dplyr::select(all_of(env_vars))

# Check for and remove missing values
env_data <- env_data %>% drop_na()
```


---

### Step 1.2: Perform PCA

**What this step does:**
Perform Principal Component Analysis using `prcomp()` with `scale. = TRUE` to standardize variables, then calculate variance explained by each component.

**Why we do this:**
Scaling is critical because bioclimatic variables have very different units and scales (temperature in °C vs precipitation in mm). Without scaling, variables with larger ranges would dominate the PCA.

**Your task:** Run PCA on the scaled environmental data and create a table showing variance explained by the first 10 PCs.

```{r PCA}
# Perform PCA with scaling
env_pca <- prcomp(env_data, scale = TRUE)

# Calculate variance explained
var_exp <- summary(env_pca)$importance[2,]*100
var_cum <- summary(env_pca)$importance[3,]*100

# Create variance table
var_table <- data.frame("Variance_explained" = var_exp, "Cumulative_variance" =var_cum) %>% head(n = 10)
print(var_table)
```


---

### Step 1.3: Determine Number of Axes to Retain

**What this step does:**
Use two methods to decide how many principal components to keep: (1) Scree plot (elbow method), and (2) Broken stick model.

**Why we do this:**
Not all PCs are equally informative. These methods help us balance simplification with information retention.

**Elbow Method:** Look for the "elbow" where variance explained drops off
**Broken Stick Model:** Keep components explaining more variance than expected by chance

**Your task:** Create scree plots and calculate the number of PCs exceeding the broken stick threshold.

```{r Determine number of PCs}
# Scree plot
plot(env_pca, type = "lines", main = "Scree Plot")

# Broken stick model function
broken_stick <- function(n) {
  res <- numeric(n)
  for (i in 1:n) {
    res[i] <- (1/n) * sum(1/(i:n))
  }
  return(res*100)
}
expected_var <- broken_stick(20)

# Compare observed vs expected
plot(1:20, var_exp, type = "b",
     xlab = "Principal Component", 
     ylab = "Variance Explained",
     main = "PCA: Observed vs Broken Stick") 
lines(1:20, expected_var, col = "red")
legend("topright", legend = c("Observed", "Broken Stick"),
       col = c("black", "red"), lty = 1)

# Count axes to retain
cat("Number of Principal components to keep (Broken stick method): ", sum(var_exp > expected_var))

```

---

### Step 1.4: Interpret Variable Loadings

**What this step does:**
Extract loadings for the first 3 PCs and identify the top 5 contributing variables for each.

**Why we do this:**
Loadings transform abstract mathematical axes into ecologically meaningful environmental gradients (e.g., "PC1 represents a temperature gradient").

**Your task:** Extract and interpret the loadings for PC1, PC2, and PC3.

```{r PCA loadings}
# Extract loadings
pca_loadings <- env_pca$rotation[,1:3]
print(round(pca_loadings, 3))

# Identify top variables for each PC
for (i in 1:3){
  cat("\nPC:",i,"\n")
  top_5 <- pca_loadings[order(abs(pca_loadings[,i]), decreasing = TRUE),i][1:5]
  top_5 <- round(top_5,3)
  print(top_5)
}

```

---

### Step 1.5: Visualize PCA Ordination

**What this step does:**
Plot sites in PCA space, colored by biome type, showing PC1 vs PC2, PC1 vs PC3, and PC2 vs PC3.

**Why we do this:**
Visualization reveals whether sites from the same biome cluster together (distinct environmental niches) or mix (overlapping tolerances).

**Your task:** Create ordination plots with biome colors and variance explained in axis labels.

```{r PCA ordinations}
# Extract PC scores
pca_scores <- scores(env_pca, choices = 1:3)

# Create color palette
biomes <- unique(data_merged$BIOME.Name)
colors <- setNames(rainbow(length(biomes)), biomes)

# Plot PC1 vs PC2, PC1 vs PC3, PC2 vs PC3
par(mfrow = c(2, 2))

plot(pca_scores[, 1], pca_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("PC1 (", round(var_exp[1], 2), "%)"),
     ylab = paste0("PC2 (", round(var_exp[2], 2), "%)"),
     main = "PCA: PC1 vs PC2")

plot(pca_scores[, 1], pca_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("PC1 (", round(var_exp[1], 2), "%)"),
     ylab = paste0("PC3 (", round(var_exp[3], 2), "%)"),
     main = "PCA: PC1 vs PC3")

plot(pca_scores[, 2], pca_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18, cex = 0.8,
     xlab = paste0("PC2 (", round(var_exp[2], 2), "%)"),
     ylab = paste0("PC3 (", round(var_exp[3], 2), "%)"),
     main = "PCA: PC2 vs PC3")
plot.new()
legend("topright", legend = biomes, col = colors, pch = 18)

```

---

### Step 1.6: Add Convex Hulls

**What this step does:**
Draw polygons enclosing all points belonging to each biome.

**Why we do this:**
Convex hulls visualize: (1) total environmental space occupied by each biome, (2) biome overlap in environmental conditions, and (3) environmental distinctiveness.

**Your task:** Add convex hulls to your PCA plots.

```{r Convex hulls}
# Function to add convex hulls
add_hulls <- function(x, y, groups, colors) {
  unique_groups <- unique(groups)
  for (group in unique_groups) {
    idx <- which(groups == group)
    hull <- chull(x[idx], y[idx])
    polygon(x[idx][hull], y[idx][hull],
            border = colors[group], lwd = 2, col = NA)
  }
}

# Plot with hulls
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Helper function to plot PCA with hulls
plot_pca <- function(x, y, pc_x, pc_y, label_x, label_y, title) {
  plot(pc_x, pc_y,
       col = colors[x],
       pch = 16, cex = 0.6,
       xlab = paste0(label_x, " (", round(var_exp[as.numeric(sub("PC", "", label_x))], 1), "%)"),
       ylab = paste0(label_y, " (", round(var_exp[as.numeric(sub("PC", "", label_y))], 1), "%)"),
       main = title)
  add_hulls(pc_x, pc_y, x[as.numeric(row.names(env_data))], colors)
}

# PC1 vs PC2
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pca_scores[, 1], pca_scores[, 2],
         "PC1", "PC2", "PCA with Convex Hulls: PC1 vs PC2")

# PC1 vs PC3
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pca_scores[, 1], pca_scores[, 3],
         "PC1", "PC3", "PCA with Convex Hulls: PC1 vs PC3")

# PC2 vs PC3
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pca_scores[, 2], pca_scores[, 3],
         "PC2", "PC3", "PCA with Convex Hulls: PC2 vs PC3")

# Legend
plot.new()
legend("topright", legend = biomes, col = colors, pch = 16)


```

---

### Reflection Questions - Task 1

**Question 1.1:** Based on the scree plot and broken stick model, how many principal components would you retain for further analysis? Explain your reasoning and discuss what percentage of the total variance is explained by these components.

*Based on the scree plot and broken stick model I would choose to use 3 principal components, because the scree plot has an elbow at 3 PCs, and in the broken stick model the observed explained variance was higher than the expected explained variance for the first 3 PCs. The first PC explained 51 % of the variance, which is considerable. The other other two PCs explained close to 15% each. Thus, as around 80% of the variance is explained by only three PCs, it is a good idea to reduce the dimensionality of the dataset using PCA.*

**Question 1.2:** Examine the variable loadings for the first two principal components. What environmental gradients do PC1 and PC2 appear to represent? Which bioclimatic variables contribute most strongly to each axis, and what does this tell you about the main environmental factors differentiating these biomes?

*PC1 represents a temperature gradient, especially focused on cold-season conditions. It’s driven by variables like minimum temperature of the coldest month, mean temperature of the coldest quarter, and annual mean temperature. Biomes with colder climates score higher on PC1, while warmer biomes score lower. PC2 reflects a precipitation gradient, particularly during dry periods. It’s influenced by precipitation of the driest month and quarter, annual precipitation, and diurnal temperature range. Biomes with more dry-season rainfall score higher, while arid or seasonally dry biomes score lower. Together, PC1 and PC2 capture the main climatic differences between biomes: temperature extremes and moisture availability.*

---

## Task 2: Principal Coordinates Analysis (PCoA)

### Background and Objectives

**What is PCoA?**

Principal Coordinates Analysis (metric multidimensional scaling) works with distance matrices rather than raw data. Unlike PCA, PCoA can handle non-linear relationships through the choice of dissimilarity measure.

**Why use Euclidean distance?**

Euclidean distance is appropriate for continuous environmental data. When used with scaled variables, it produces results similar to PCA but demonstrates the distance-based framework.

**What we will do:**

1. Scale environmental variables
2. Calculate Euclidean distance matrix
3. Perform PCoA on the distance matrix
4. Determine how many PCo axes to retain
5. Visualize the ordination
6. Add convex hulls

---

### Step 2.1: Scale Environmental Data

**What this step does:**
Standardize each environmental variable to mean = 0 and standard deviation = 1 using `scale()`.

**Why we do this:**
Scaling ensures all variables contribute equally to distance calculations based on ecological variation, not measurement units.

**Your task:** Scale the environmental data and verify the transformation.

```{r Scale variables}
# Scale environmental variables
env_data_scaled <- scale(env_data)

# Verify scaling (check means and SDs)
cat("First 10 scaled means: ", round(colMeans(env_data_scaled)[1:10]))

cat("First 10 scaled standard deviations: ", round(apply(env_data_scaled, 2, sd)[1:10]))

```

---

### Step 2.2: Calculate Euclidean Distance

**What this step does:**
Calculate Euclidean distance between every pair of sites using `dist()`.

**Why we do this:**
The distance matrix quantifies environmental similarity. Sites with small distances have similar conditions; large distances indicate different conditions.

**Your task:** Calculate the Euclidean distance matrix and examine its properties.

```{r Euclidean distances}
# Calculate distance matrix
dist_matrix <- dist(env_data_scaled, method = "euclidean")

# Display summary statistics
cat("Summary of distance matrix:\n")
print(summary(dist_matrix))

cat("\nDimension of distance matrix:\n")
print(dim(as.matrix(dist_matrix)))

```

---

### Step 2.3: Perform PCoA

**What this step does:**
Use `cmdscale()` to perform PCoA, requesting 20 dimensions and eigenvalues, then calculate variance explained.

**Why we do this:**
PCoA transforms distances into an ordination space for visualization. With Euclidean distance on scaled data, PCoA is mathematically equivalent to PCA.

**Your task:** Perform PCoA and create a variance table for the first 10 axes.

```{r}
# Perform PCoA
pcoa <- cmdscale(dist_matrix, k = 20, eig = TRUE)

# Calculate variance explained by positive eigenvalues
pos_eigen_vals <- pcoa$eig[pcoa$eig >= 0][1:20]
var_exp_pcoa <- (pos_eigen_vals/sum(pos_eigen_vals))*100
cum_exp_pcoa <- cumsum(var_exp_pcoa)

# Create variance table
var_table_pcoa <- data.frame("Variance_explained" = var_exp_pcoa, 
                             "Cumulative_explained" = cum_exp_pcoa)

```

---

### Step 2.4: Determine Number of Axes to Retain

**What this step does:**
Use scree plot and broken stick model to determine how many PCoA axes to retain.

**Why we do this:**
Focus on meaningful axes representing major environmental gradients rather than noise.

**Your task:** Create diagnostic plots and count axes exceeding the broken stick threshold.

```{r}
par(mfrow=c(1,1))
# Scree plot
plot(1:20, var_exp_pcoa[1:20], 
     type = "b",
     main = "PCoA Scree Plot",
     xlab = "Principal Coordinate", 
     ylab = "Variance Explained (%)")

# Broken stick
expected_var_pcoa <- broken_stick(20)

# Compare observed vs expected
plot(1:20, var_exp_pcoa, type = "b",
     xlab = "Principal Coordinate", 
     ylab = "Variance Explained",
     main = "PCoA: Observed vs Broken Stick") 
lines(1:20, expected_var_pcoa, col = "red")
legend("topright", legend = c("Observed", "Broken Stick"),
       col = c("black", "red"), lty = 1)

# Count axes to retain
cat("Number of Principal coordinates to keep (Broken stick method): ", sum(var_exp_pcoa > expected_var_pcoa))



```

---

### Step 2.5: Visualize PCoA Ordination

**What this step does:**
Plot sites in PCoA space, colored by biome, showing PCo1 vs PCo2, PCo1 vs PCo3, and PCo2 vs PCo3.

**Why we do this:**
Assess whether sites from the same biome cluster together in environmental space.

**Your task:** Create PCoA ordination plots with biome colors.

```{r}
# Extract PC scores
pcoa_scores <- scores(pcoa, choices = 1:3)

# Plot PCo1 vs PCo2, PCo1 vs PCo3, PCo2 vs PCo3
par(mfrow = c(2, 2))

plot(pcoa_scores[, 1], pcoa_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("PCo1 (", round(var_exp_pcoa[1], 2), "%)"),
     ylab = paste0("PCo2 (", round(var_exp_pcoa[2], 2), "%)"),
     main = "PCoA: PCo1 vs PCo2")

plot(pcoa_scores[, 1], pcoa_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("PCo1 (", round(var_exp_pcoa[1], 2), "%)"),
     ylab = paste0("PCo3 (", round(var_exp_pcoa[3], 2), "%)"),
     main = "PCoA: PCo1 vs PCo3")

plot(pcoa_scores[, 2], pcoa_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18, cex = 0.8,
     xlab = paste0("PCo2 (", round(var_exp_pcoa[2], 2), "%)"),
     ylab = paste0("PCo3 (", round(var_exp_pcoa[3], 2), "%)"),
     main = "PCoA: PCo2 vs PCo3")
plot.new()
legend("topright", legend = biomes, col = colors, pch = 18)

```

---

### Step 2.6: Add Convex Hulls to PCoA

**What this step does:**
Draw convex hulls around each biome in PCoA space.

**Why we do this:**
Compare environmental space occupied by biomes. Since we used Euclidean distance on scaled data, hulls should closely resemble PCA hulls.

**Your task:** Add convex hulls to PCoA plots.

```{r PCoA with convex hulls}
# Helper function to plot PCA with hulls
plot_pcoa <- function(x, y, pc_x, pc_y, label_x, label_y, title) {
  plot(pc_x, pc_y,
       col = colors[x],
       pch = 16, cex = 0.6,
       xlab = paste0(label_x, " (", round(var_exp_pcoa[as.numeric(sub("PCo", "", label_x))], 1), "%)"),
       ylab = paste0(label_y, " (", round(var_exp_pcoa[as.numeric(sub("PCo", "", label_y))], 1), "%)"),
       main = title)
  add_hulls(pc_x, pc_y, x[as.numeric(row.names(env_data))], colors)
}

# Plot with convex hulls
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# PC1 vs PC2
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pcoa_scores[, 1], pcoa_scores[, 2],
         "PCo1", "PCo2", "PCoA with Convex Hulls: PCo1 vs PCo2")

# PC1 vs PC3
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pcoa_scores[, 1], pcoa_scores[, 3],
         "PCo1", "PCo3", "PCoA with Convex Hulls: PCo1 vs PCo3")

# PC2 vs PC3
plot_pca(data_merged$BIOME.Name, data_merged$BIOME.Name,
         pcoa_scores[, 2], pcoa_scores[, 3],
         "PCo2", "PCo3", "PCoA with Convex Hulls: PCo2 vs PCo3")

# Legend
plot.new()
legend("topright", legend = biomes, col = colors, pch = 16)

```

---

### Reflection Questions - Task 2

**Question 2.1:** Compare the PCoA ordination to the PCA ordination. Since we used Euclidean distance on scaled data, the results should be mathematically equivalent (though possibly reflected or rotated). Do you observe this equivalence? What does this tell you about the relationship between PCA and distance-based ordination methods?

*Yes, the PCoA ordinations appears mathematically equivalent to the PCA ordinations, as expected when using Euclidean distance on scaled data. The plots differ in orientation however, but the relative positions of points and groupings are consistent. The variance explained by the axes are also similar. This confirms that PCA is a special case of distance-based ordination methods like PCoA when Euclidean distances are used. It shows that both methods capture the same structure in the data, but PCoA offers more flexibility with other distance metrics.*

**Question 2.2:** Looking at the convex hulls in the PCoA plots, which biomes show the most overlap in environmental space? Which are most distinct? What might this tell you about the environmental specificity of different vegetation types?

*In the PCoA plots with convex hulls, some biomes show clear overlap—such as temperate forest and temperate grassland—indicating similar environmental conditions. Others, like desert and tundra, are more distinct, suggesting strong environmental specificity. This implies that some vegetation types occupy broad, overlapping climate spaces, while others are more narrowly adapted to specific conditions.*

---

## Task 3: Non-metric Multidimensional Scaling (NMDS)

### Background and Objectives

**What is NMDS?**

Non-metric Multidimensional Scaling uses rank orders rather than absolute distances, making it robust to non-linear relationships.

**Understanding Stress:**

Stress measures goodness-of-fit:
- Stress < 0.05: Excellent
- Stress < 0.10: Good
- Stress < 0.20: Acceptable
- Stress > 0.20: Poor (use more dimensions)

**What we will do:**

1. Load the vegan package
2. Perform NMDS with different numbers of dimensions
3. Plot stress vs dimensionality
4. Select optimal dimensions
5. Visualize the ordination
6. Add convex hulls

---

### Step 3.1: Run NMDS with Different Dimensions

**What this step does:**
Run NMDS using 2, 3, 4, and 5 dimensions with `metaMDS()`, performing multiple random starts for each.

**Why we do this:**
Test different dimensionalities to find the optimal balance between simplicity and fit quality. Multiple random starts avoid local optima.

**Your task:** Run NMDS for k = 2, 3, 4, 5 and record stress values.

```{r NMDS}
# Load vegan package
library(vegan)

# Run NMDS for different dimensions
stress_scores <- data.frame("Dimensions" = numeric(),"Stress" = numeric())
for(i in 2:5){
  nmds <- metaMDS(dist_matrix, k = i, trace = FALSE,
                                autotransform = FALSE,
                                trymax = 20)
  stress_scores[i-1,1] <- i
  stress_scores[i-1,2] <- nmds$stress
}

```

---

### Step 3.2: Plot Stress vs Number of Dimensions

**What this step does:**
Create a stress plot with reference lines at 0.05, 0.10, and 0.20.

**Why we do this:**
Identify minimum dimensions needed for acceptable fit, balancing stress reduction with interpretability.

**Your task:** Create stress plot and identify optimal dimensionality.

```{r Plot stress vs dimensions}
# Plot stress vs dimensions
plot(stress_scores, type = "b", ylim = c(0, max(stress_scores$Stress)*1.5))

# Add reference lines
abline(h = 0.20, lty = 2)
abline(h = 0.10, lty = 2)
abline(h = 0.05, lty = 2)

# Add labels
text(max(stress_scores$Dimensions), 0.20, pos = 2, "Acceptable (0.20)")
text(max(stress_scores$Dimensions), 0.10, pos = 2, "Good (0.10)")
text(max(stress_scores$Dimensions), 0.05, pos = 2, "Excellent (0.05)")

best_nmds <- metaMDS(dist_matrix, k = 3, trace = FALSE,
                                autotransform = FALSE,
                                trymax = 20)

```

---

### Step 3.3: Visualize NMDS Ordination

**What this step does:**
Plot NMDS axes 1 vs 2, 1 vs 3, and 2 vs 3, colored by biome.

**Why we do this:**
Assess whether NMDS successfully separates biomes based on environmental similarity.

**Important differences from PCA/PCoA:**
- No variance explained (all axes equally important)
- Arbitrary rotation (only relative positions matter)
- Rank-based (robust to non-linear relationships)

**Your task:** Create NMDS ordination plots.

```{r NMDS ordination}
# Extract NMDS scores
nmds_scores <- scores(best_nmds, choices = 1:3)

# Plot NMDS1 vs NMDS2, NMDS1 vs NMDS3, NMDS2 vs NMDS3
par(mfrow = c(2, 2))

plot(nmds_scores[, 1], nmds_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("NMDS1"),
     ylab = paste0("NMDS2"),
     main = "NMDS: NMDS1 vs NMDS2")

plot(nmds_scores[, 1], nmds_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18,
     xlab = paste0("NMDS1"),
     ylab = paste0("NMDS3"),
     main = "NMDS: NMDS1 vs NMDS3")

plot(nmds_scores[, 2], nmds_scores[, 3],
     col = colors[data_merged$BIOME.Name],
     pch = 18, cex = 0.8,
     xlab = paste0("NMDS2"),
     ylab = paste0("NMDS3"),
     main = "NMDS: NMDS2 vs NMDS3")
plot.new()
legend("topright", legend = biomes, col = colors, pch = 18)

```

---

### Step 3.4: Add Convex Hulls to NMDS

**What this step does:**
Draw convex hulls around each biome in NMDS space.

**Why we do this:**
Compare biome separation across all three ordination methods to understand which captures biome-climate relationships best.

**Your task:** Add convex hulls to NMDS plots and compare with PCA and PCoA results.

```{r NMDS with convex hulls}
# Plot with convex hulls
# Helper function to plot PCA with hulls
plot_nmds <- function(x, y, pc_x, pc_y, label_x, label_y, title) {
  plot(pc_x, pc_y,
       col = colors[x],
       pch = 16, cex = 0.6,
       xlab = label_x,
       ylab = label_y,
       main = title)
  add_hulls(pc_x, pc_y, x[as.numeric(row.names(env_data))], colors)
}

# Plot with convex hulls
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# PC1 vs PC2
plot_nmds(data_merged$BIOME.Name, data_merged$BIOME.Name,
         nmds_scores[, 1], nmds_scores[, 2],
         "NMDS1", "NMDS2", "NMDS with Convex Hulls: NMDS1 vs NMDS2")

# PC1 vs PC3
plot_nmds(data_merged$BIOME.Name, data_merged$BIOME.Name,
         nmds_scores[, 1], nmds_scores[, 3],
         "NMDS1", "NMDS3", "NMDS with Convex Hulls: NMDS1 vs NMDS3")

# PC2 vs PC3
plot_nmds(data_merged$BIOME.Name, data_merged$BIOME.Name,
         nmds_scores[, 2], nmds_scores[, 3],
         "NMDS2", "NMDS3", "NMDS with Convex Hulls: NMDS2 vs NMDS3")

# Legend
plot.new()
legend("topright", legend = biomes, col = colors, pch = 16)

```

---

### Reflection Questions - Task 3

**Question 3.1:** Based on the stress values you calculated, which number of dimensions provides the best balance between model complexity and goodness-of-fit? Would you consider the representation with your chosen number of dimensions to be excellent, good, or acceptable according to standard stress guidelines?

*A three-dimensional NMDS solution provides the best balance between model simplicity and goodness-of-fit. The stress value of 0.066 falls within the range considered good according to standard guidelines (stress < 0.1). While adding more dimensions further reduces stress, the improvement becomes marginal, and three dimensions are sufficient for a reliable representation without overcomplicating the model.*

**Question 3.2:** Compare the three ordination methods (PCA, PCoA, and NMDS) in terms of how clearly they separate the different biomes. Which method appears most effective for distinguishing vegetation types? Why might different ordination techniques produce different patterns?

*Among PCA, PCoA, and NMDS, NMDS separates the biomes a bit better, especially when looking at the first to dimensions of all plots. PCA and PCoA produce similar patterns due to their reliance on Euclidean distances, but NMDS captures nonlinear relationships and better reflects ecological dissimilarities. Different ordination methods emphasize different aspects—PCA focuses on variance, PCoA on distances, and NMDS on rank-order dissimilarities—leading to variations in how biomes are grouped and distinguished.*

---

## Task 4: Non-hierarchical Classification (K-means Clustering)

### Background and Objectives

**What is K-means clustering?**

K-means partitions objects into k clusters by minimizing within-cluster variation through iterative assignment and centroid recalculation.

**Why use ordination spaces?**

Ordination reduces noise and collinearity, producing more interpretable and stable clusters than the original high-dimensional space.

**What we will do:**

1. Apply K-means to PCA, PCoA, and NMDS spaces
2. Compare classifications across methods
3. Determine optimal number of clusters
4. Assess classification accuracy

---

### Step 4.1: K-means on All Three Ordination Spaces

**What this step does:**
Apply K-means clustering to PCA, PCoA, and NMDS spaces using k = number of known biomes.

**Why we do this:**
Compare which ordination technique best captures natural groupings in climate-vegetation data.

**Key outputs:**
- Total within-cluster SS (lower = more compact clusters)
- Between-cluster SS (higher = more separated clusters)
- Ratio (higher = better-defined clusters)

**Your task:** Run K-means on all three ordination spaces and compare results.

```{r K-means clustering}
# K-means on PCA space 
set.seed(123)
kmeans_pca <- kmeans(pca_scores[,1:3], centers = length(biomes), nstart = 20)

# K-means on PCoA space
set.seed(123)
kmeans_pcoa <- kmeans(pcoa_scores[,1:3], centers = length(biomes), nstart = 20)

# K-means on NMDS space (using 5 dimensions)
set.seed(123)
kmeans_nmds <- kmeans(nmds_scores, centers = length(biomes), nstart = 20)

# Summary of results
cat("Summary for kmeans_pca\n")
summary(kmeans_pca)
cat("\nSummary for kmeans_pcoa\n")
summary(kmeans_pcoa)
cat("\nSummary for kmeans_nmdsn\n")
summary(kmeans_nmds)



```

---

### Step 4.2: Visualize K-means Classifications

**What this step does:**
Create side-by-side plots showing original biomes vs K-means clusters for each ordination method.

**Why we do this:**
Visually assess how well unsupervised clustering recovers known biome patterns.

**Your task:** Create comparison plots with cluster centroids marked.

```{r Plot clusters}
par(mfrow = c(3, 2), mar = c(4, 4, 3, 1))

# PCA - original biomes
plot(pca_scores[, 1], pca_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 16, cex = 0.8,
     xlab = "PC1", ylab = "PC2",
     main = "PCA: Original Biomes")

# PCA - K-means clusters
plot(pca_scores[, 1], pca_scores[, 2],
     col = kmeans_pca$cluster,
     pch = 16, cex = 0.8,
     xlab = "PC1", ylab = "PC2",
     main = "PCA: K-means Clusters")
points(kmeans_pca$centers[, 1], kmeans_pca$centers[, 2],
       pch = 8, cex = 2, lwd = 2, col = "black")

# PCoA - original biomes
plot(pcoa_scores[, 1], pcoa_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 16, cex = 0.8,
     xlab = "PCo1", ylab = "PCo2",
     main = "PCoA: Original Biomes")

# PCoA - K-means clusters
plot(pcoa_scores[, 1], pcoa_scores[, 2],
     col = kmeans_pcoa$cluster,
     pch = 16, cex = 0.8,
     xlab = "PCo1", ylab = "PCo2",
     main = "PCoA: K-means Clusters")
points(kmeans_pcoa$centers[, 1], kmeans_pcoa$centers[, 2],
       pch = 8, cex = 2, lwd = 2, col = "black")

# NMDS - original biomes
plot(nmds_scores[, 1], nmds_scores[, 2],
     col = colors[data_merged$BIOME.Name],
     pch = 16, cex = 0.8,
     xlab = "NMDS1", ylab = "NMDS2",
     main = "NMDS: Original Biomes")

# NMDS - K-means clusters
plot(nmds_scores[, 1], nmds_scores[, 2],
     col = kmeans_nmds$cluster,
     pch = 16, cex = 0.8,
     xlab = "NMDS1", ylab = "NMDS2",
     main = "NMDS: K-means Clusters")
points(kmeans_nmds$centers[, 1], kmeans_nmds$centers[, 2],
       pch = 8, cex = 2, lwd = 2, col = "black")


```

---

### Step 4.3: Determine Optimal Number of Clusters

**What this step does:**
Test k = 2 to 15 using: (1) Elbow method (within-cluster SS), and (2) Silhouette method (cluster quality).

**Why we do this:**
The true number of ecological groups may differ from named biomes. Find the number best fitting data structure.

**Your task:** Create elbow and silhouette plots, identify optimal k.

```{r Optimal k}
# Test k = 2:15
k_values <- 2:15

# Initialize results
results <- lapply(k_values, function(k) {
  km <- kmeans(pcoa_scores[, 1:3], centers = k, nstart = 20)
  
  # Within-cluster sum of squares
  wss <- km$tot.withinss
  
  # Silhouette score calculation
  clusters <- km$cluster
  sil_scores <- sapply(1:nrow(pcoa_scores), function(i) {
    point <- pcoa_scores[i, 1:3]
    own <- pcoa_scores[clusters == clusters[i], 1:3]
    a <- mean(sqrt(rowSums((t(t(own) - point))^2)))
    
    other_clusters <- setdiff(unique(clusters), clusters[i])
    b <- min(sapply(other_clusters, function(c) {
      other <- pcoa_scores[clusters == c, 1:3]
      mean(sqrt(rowSums((t(t(other) - point))^2)))
    }))
    
    (b - a) / max(a, b)
  })
  
  list(k = k, wss = wss, silhouette = mean(sil_scores))
})

# Extract values
wss <- sapply(results, `[[`, "wss")
silhouette_scores <- sapply(results, `[[`, "silhouette")

# Plot results
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2))

plot(k_values, wss, type = "b", pch = 19, col = "blue",
     xlab = "Number of Clusters (k)",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
grid()

plot(k_values, silhouette_scores, type = "b", pch = 19, col = "darkgreen",
     xlab = "Number of Clusters (k)",
     ylab = "Average Silhouette Score",
     main = "Silhouette Method")
abline(h = 0, lty = 2, col = "red")
grid()

```

---

### Step 4.4: Run K-means with Optimal Clusters

**What this step does:**
Perform K-means with optimal k and create confusion matrix comparing clusters to actual biomes.

**Why we do this:**
Compare data-driven clustering with expert-defined biome system.

**Your task:** Run optimal K-means and examine confusion matrix.

```{r K-means with optimal k}
# K-means with optimal k
optimal_k <- 4  

# Run K-means
km_final <- kmeans(pcoa_scores[, 1:3], centers = optimal_k, nstart = 20)

# Visualize clustering result
plot(pcoa_scores[, 1], pcoa_scores[, 2],
     col = km_final$cluster,
     pch = 16,
     xlab = "PCoA Axis 1",
     ylab = "PCoA Axis 2",
     main = paste("K-means Clustering (k =", optimal_k, ")"))

# Compare clusters to actual biomes
actual_biomes <- data_merged$BIOME.Name[as.numeric(row.names(env_data))]
cluster_labels <- km_final$cluster

# Create confusion matrix
conf_matrix <- table(Cluster = cluster_labels, Biome = actual_biomes)
print(conf_matrix)


```

---

### Step 4.5: Assess Classification Accuracy

**What this step does:**
Calculate accuracy for each clustering method by matching clusters to biomes.

**Why we do this:**
Quantify agreement between unsupervised clustering and expert-defined biomes.

**Your task:** Calculate and compare accuracies across all methods.

```{r Accuracy across methods}
# Function to calculate clustering accuracy
calculate_accuracy <- function(cluster_labels, true_labels) {
  # Create confusion matrix
  conf_matrix <- table(cluster_labels, true_labels)
  
  # Match clusters to biomes using majority vote
  correct_matches <- sum(apply(conf_matrix, 1, function(row) max(row)))
  
  # Calculate accuracy
  accuracy <- correct_matches / length(true_labels)
  return(accuracy)
}

# Prepare true labels (subset to match clustering input)
true_biomes <- data_merged$BIOME.Name[as.numeric(row.names(env_data))]

# Calculate accuracies
acc_pca <- calculate_accuracy(kmeans_pca$cluster, data_merged$BIOME.Name[as.numeric(row.names(env_data))])
acc_pcoa <- calculate_accuracy(kmeans_pcoa$cluster, data_merged$BIOME.Name[as.numeric(row.names(env_data))])
acc_nmds <- calculate_accuracy(kmeans_nmds$cluster, data_merged$BIOME.Name[as.numeric(row.names(env_data))])
acc_optimal <- calculate_accuracy(km_final$cluster, data_merged$BIOME.Name[as.numeric(row.names(env_data))])

# Print all results
cat("Clustering Accuracy Results:\n")
cat(" - PCA-based K-means:     ", round(acc_pca * 100, 2), "%\n")
cat(" - PCoA-based K-means:    ", round(acc_pcoa * 100, 2), "%\n")
cat(" - NMDS-based K-means:    ", round(acc_nmds * 100, 2), "%\n")
cat(" - Optimal K-means (PCoA):", round(acc_optimal * 100, 2), "%\n")
```

---

### Reflection Questions - Task 4

**Question 4.1:** Compare the classification accuracies across the three ordination methods (PCA, PCoA, NMDS). Which ordination space produced the most accurate K-means clustering? Why might certain ordination methods be more suitable for classification tasks than others?

*PCA, PCoA, and NMDS, had similar clustering accuracies (~41%), suggesting they capture comparable structure in the environmental data. NMDS performed slightly better, likely due to its ability to preserve rank-based dissimilarities. In contrast, the optimal clustering based on the silhouette method (k = 4) resulted in lower accuracy (~23%), indicating that the most statistically distinct clusters do not necessarily align with defined biomes. This highlights that ordination methods emphasizing ecological distances (like NMDS) may be more suitable for classification tasks than those relying strictly on Euclidean space and linearity.*

**Question 4.2:** You determined an optimal number of clusters using the silhouette method, which may differ from the actual number of biomes in the dataset. What does this suggest about the natural structure of vegetation types based on climate data? Discuss whether biomes are truly discrete categories or whether they represent points along continuous environmental gradients.

*That the optimal number of clusters differs from the actual number of biomes suggests that biomes may not be strictly discrete entities. Instead, they likely represent positions along continuous environmental gradients, shaped by overlapping climate conditions. This supports the idea that vegetation types changes gradually across climate space, and that clustering based on environmental data may reveal natural groupings that differ from human-defined biome boundaries.*

---

## Summary and Conclusions

### Key Findings

Today you have learned to:

1. **Apply ordination techniques** - PCA, PCoA, and NMDS each reveal different aspects of environmental structure
2. **Determine dimensionality** - Using scree plots, broken stick models, and stress values
3. **Classify ecological data** - K-means clustering identifies natural groupings
4. **Validate classifications** - Accuracy metrics assess whether groups are meaningful

### Take-Home Messages

- **No single best method**: Different techniques have different strengths
- **Dimensionality matters**: Balance information retention with simplification
- **Classification is challenging**: Ecosystems exist along continua, not in discrete boxes
- **Statistical validation is essential**: Always test significance and evaluate accuracy

### Further Exploration

Consider these extensions:

- Predict vegetation types at unsampled locations
- Apply hierarchical clustering and compare to K-means
- Identify key variables discriminating specific biome types
- Explore machine learning approaches for classification