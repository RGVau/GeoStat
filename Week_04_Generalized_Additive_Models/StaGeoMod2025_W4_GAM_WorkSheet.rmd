---
title: "Hands-on Practical: Generalised Additive Models (GAMs) with Bioluminescence"
subtitle: "StatGeoMod2025 — 1.5-hour practical"
author: "René Gier Vemmelund"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    self_contained: yes
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

## Setup

```{r}
#| label: setup
#| include: true
#| eval: true
#| echo: false
#| message: false

# Runng options
knitr::opts_chunk$set(echo = TRUE,error=TRUE)

# Packages
library(tidyverse)
library(mgcv)
library(broom)
library(patchwork)

# Data: ISIT 
ISIT <-  read_table("DATA/ISIT.txt")

# For reproducibility
set.seed(123)




```

## Part A — Why GAMs? (Basics & intuition) (10 min)

::: {.alert .alert-info}
**Task A1 (concept).**

A GAM replaces a single global slope with a **smooth function**
$f(x): Y_i = \alpha + f(X_i) + \varepsilon_i$.

In `mgcv`, `s(x)` builds $f(x)$ from spline bases, and smoothness is
selected automatically (REML/GCV).
:::

::: {.alert .alert-info}
**Task A2 (visual).**

**What you’ll do (and why):**

1.  **Prepare the data.** Select only Station 16 from the ISIT dataset
    and create a clean dataframe with three variables: `Station`,
    `Depth`, and `Sources`.\
2.  **Inspect the structure.** Use `glimpse()` to confirm that `Depth`
    is numeric (continuous predictor) and `Sources` is count data
    (non-negative integers). This step helps you check if the data type
    matches the modelling assumptions later.\
3.  **Visualise.** Make a scatterplot of Sources vs Depth using
    `ggplot2`. Adjust the transparency (`alpha = 0.7`) to help visualise
    overlapping points. Add informative labels and a minimal theme.\
4.  **Interpret.** After plotting, pause to describe what you see:
    -   Is the pattern approximately linear, or clearly non-linear?\
    -   Are there regions of the depth gradient where Sources increase,
        peak, or decline?\
    -   Would fitting a straight line capture the overall shape, or
        would it miss important features?

This visual exploration sets the stage for why GAMs are needed: we often
encounter curved, irregular, or locally varying relationships that
polynomials or linear fits cannot capture well.
:::

```{r}
## Fill in the blanks (`___`) in the code below.
#| label: A2-plot
# Prepare Station 16
dat <- ISIT |>
  dplyr::filter(Station == 16) |>
  transmute(Station,
            Depth   = SampleDepth,
            Sources = Sources)

# Quick look
glimpse(dat)

# Visualise
ggplot(dat, aes(Depth, Sources)) +
  geom_point(alpha = 0.7) +
  labs(title = "Bioluminescence vs Depth (Station 16)",
       y = "Bioluminescence") +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**.

-   What overall pattern do you see in the scatterplot?

I see that Bioluminescence decreases as Depth increases.

-   If you fitted a straight line here, what would it capture well, and
    what would it miss?
    
It would capture the overall negative trend, however it would not capture that there is a threshold at Depth ~ 2500-2750, where the slope increases to ~0.    
    
-   Can you identify parts of the depth range where the relationship
    changes direction (e.g., rises then falls)?
    
Yes, there is a clear threshold at Depth at 2500-2750 where the slope changes. However, at Depth range 1000-1500 there also is a hump.
    
-   How might this motivate the need for flexible smoothers in GAMs?

That there is a possible hump suggests that the GAM might fit better with a more flexible smoother (adds more wiggliness).
:::

## Part B — First GAM vs Polynomial (Runge motivation) (20 min)

::: {.alert .alert-info}
**Task B1 — Fit your first GAM (cubic regression spline)**

**What you’ll do (and why):**

-   **Model:** Fit a GAM to `Sources ~ s(Depth)` using a **cubic
    regression spline** (`bs = "cr"`).\
-   **Smoothing selection:** Use **REML** (`method = "REML"`) so the
    model estimates the amount of smoothing (λ) automatically—balancing
    wiggliness and over-penalisation.\
-   **Complexity (edf):** Inspect **effective degrees of freedom (edf)**
    in `summary()` to gauge curve flexibility ($\approx$ 1 is almost
    linear; larger edf = more curvature).\
-   **Diagnostics:** Use `gam.check()` to examine residual patterns, QQ
    plot, scale–location, and the **k-index** (basis adequacy). A **low
    k-index** suggests increasing `k` (the basis dimension) and
    refitting.\
-   **Outcome:** You’ll see how GAMs adapt to the clear non-linearity in
    the bioluminescence vs depth relationship (Station 16) without
    resorting to high-degree polynomials.

**Hints for robust modelling:** - Start with the default `k` (mgcv
chooses a sensible basis size), then **only** increase it if
`gam.check()` warns (k-index \< \~1).\
- Prefer **REML** over GCV for smoother, more stable fits in many
practical settings.\
- Always validate: smooth fit $\neq$ good residuals—check both.
:::

```{r}
#| label: B1-gam-student
#| echo: true
# Fill in the blanks (___) and then set eval: true to run
# Goal: Fit a GAM with a cubic regression spline and inspect diagnostics

# 1) Fit the model (use REML for smoothing selection)
m_gam <- gam(Sources ~ s(Depth, bs = "cr"), data = dat, method = "REML")

# 2) Summarise: check edf, significance of s(), adjusted R^2, and REML score
summary(m_gam)

# 3) Diagnostics: residual patterns and k-index
par(mfrow = c(2, 2))
gam.check(m_gam)
par(mfrow = c(1, 1))
```

::: {.alert .alert-success}
**Question**

-   **Shape check:** What does the fitted smooth suggest about how
    `Sources` changes with `Depth` (e.g., monotonic, hump-shaped,
    thresholds)?\
    
That the fitted smooth have a significant (p-value < 0.05) effect and that Sources changes both hump-shaped and with a threshold.
    
-   **Complexity:** What is the reported **edf**, and how would you
    interpret that value in plain language?\
    
edf was 8.518. That edf >> 1 suggests that the fitted gam suggests considerable wiggliness in the line.
    
-   **Adequacy:** Does `gam.check()` indicate any **patterned
    residuals** or **basis inadequacy** (k-index warnings)? If yes, what
    would you try next?\
    
The p-value was 0.58 and thus I would not assess the k to be too low. The deviance residuals, however dose not fit a straight line well, and so the residuals are not normally distributed. There also seems to be heteroscedasticity looking at the linear predictor vs residuals plot. I would try and fix this with a Poisson or nb family for the errors.    
    
-   **Comparative lens:** In what ways would a polynomial of degree 4–6
    likely behave differently at the **edges** of the `Depth` range
    (Runge-type behaviour)?\
    
The fitted line would probably deviate a lot at the edges (low and high Depth) compared to the points.
:::

::: {.alert .alert-info}
**Task B2 — Plot the GAM on the response scale with uncertainty**

**What you’ll do (and why):**

-   **Purpose:** Turn the fitted GAM into an interpretable plot on the
    **response scale**, so students see predicted *Sources* against
    *Depth* in real units.
-   **Prediction grid:** Create a fine, evenly spaced depth grid to
    avoid jagged lines from irregular sampling.
-   **Uncertainty:** Use
    `predict(..., se.fit = TRUE, type = "response")` to obtain fitted
    values and their **standard errors** on the response scale. Add a
    ribbon for an approximate **95% pointwise interval** using
    `fit ± 2 * se.fit`.
-   **Visual clarity:** Overlay the fitted line and ribbon on the raw
    data to contrast modelled trend vs. observed scatter.

**Key ideas:** - `type = "response"` ensures predictions are on the
scale of *Sources* (not the link/linear predictor). - The ribbon is
**pointwise**, not simultaneous; it’s excellent for teaching
uncertainty, but do not over-interpret it as a joint interval over the
whole curve. - Dense grids (e.g., 300–500 points) give smooth lines
without oversmoothing the model itself.
:::

```{r}
#| label: B2-plot-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Plot the GAM fit on the response scale with ~95% pointwise intervals

# 1) Create a prediction grid over Depth
newd <- tibble(Depth = seq(min(dat$Depth), max(dat$Depth), length.out = 500))

# 2) Predict on the response scale with standard errors
pred_gam <- bind_cols(
  newd,
  as_tibble(predict(m_gam, newd, se.fit = TRUE, type = "response"))
)

# 3) Plot: points, fitted line, and ribbon for uncertainty
ggplot(dat, aes(Depth, Sources)) +
  geom_point(alpha = 0.7) +
  geom_line(data = pred_gam, aes(y = fit), linewidth = 1) +
  geom_ribbon(data = pred_gam,
              aes(y = fit,
                  ymin = fit - 2*se.fit,
                  ymax = fit + 2*se.fit),
              alpha = 0.2) +
  labs(title = "GAM fit: cubic regression spline (mgcv::gam)",
       subtitle = "Line = fitted; band $\approx$ 95% pointwise interval",
       y = "Bioluminescence") +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **Scale check:** Why is `type = "response"` preferable here compared
    to `type = "link"`?\
    
It is a parameter in the function predict() that makes the predictions on the scale of the response instead of the scale of the link function. In this example it doesn't make a difference as the link function is "identity".

-   **Uncertainty:** What does the shaded band represent, and where does
    it widen most? Why might that be?\
    
It indicates the uncertainty of the prediction. It widens most at the hump shape and at Depth > 2000. It might be because there are clearer trends at Depth 0-1000 and 1750-2000, so the model is more caertain.
    
-   **Edge behaviour:** Do you see wider intervals near minimum/maximum
    depth? How does sampling density play a role?\
    
At maximum depth there is a wide interval. This might be due to the sampling is climped more together at Depth > 3500.
    
-   **Interpretation:** Identify depth ranges where the fitted curve
    rises, plateaus, or falls. How would a straight-line model mislead
    here?
    
Bioluminescence decreases at Depth 0-1000 and 1500-3000. Increases from 1000-1500. Plateaus at Depth >3000. A linear model would not capture this.
:::

::: {.alert .alert-info}
**Task B3 — Compare GAM with polynomial regressions (degrees 2–6)**

**What you’ll do (and why):**

-   **Purpose:** Fit a set of **global polynomial** regressions (degrees
    2–6) and compare them with the GAM fit from Task B1–B2.\
-   **Focus:** Observe **Runge’s phenomenon** — polynomials can
    overshoot/oscillate (especially near boundaries) when trying to
    capture curved relationships.\
-   **Method:** For each degree, fit
    `lm(Sources ~ poly(Depth, degree, raw = TRUE))`, predict over the
    same grid `newd` used for the GAM, and collect fitted values/SEs.\
-   **Visual:** Overlay the polynomial fits on the raw data to compare
    shapes and **edge behaviour** with the GAM curve from B2.

**Key ideas:** - **Global vs local flexibility:** Global polynomials
force one equation to capture the entire curve; GAM smooths adapt
locally with a penalty on wiggliness.\
- **Edge effects:** Extrapolation/oscillation at the **depth extremes**
is common with higher-degree polynomials (Runge-type behaviour).\
- **Model selection:** AIC can be computed for each polynomial, but
shape realism and diagnostics matter just as much as a single metric.
:::

```{r}
#| label: B3-poly-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit polynomial regressions (degrees 2–6) and compare shapes to the GAM

# 0) Ensure you have a prediction grid (from Task B2); if not, create it:
# newd <- tibble(Depth = seq(min(___$___), max(___$___), length.out = ___))

# 1) Choose degrees to compare
deg_seq <- 2:6   # e.g., 2:6

# 2) Fit polynomials, predict on 'newd', and collect results
fits_poly <- map_df(deg_seq, ~{
  fm   <- lm(Sources ~ poly(Depth, .x, raw = TRUE), data = dat)
  pred <- predict(fm, newd, se.fit = TRUE)
  tibble(
    Depth = newd$Depth,
    fit   = as.numeric(pred$fit),
    se    = pred$se.fit,
    deg   = paste0("poly d=", .x),
    AIC   = AIC(fm)[1]
  )
})

# 3) Plot: raw data + polynomial fits
ggplot(dat, aes(Depth, Sources)) +
  geom_point(alpha = 0.7) +
  geom_line(data = fits_poly, aes(y = fit, colour = deg), linewidth = 0.7, alpha = 0.8) +
  labs(title = "Polynomial fits of increasing degree",
       subtitle = "Watch for overshoot/oscillation at the edges (Runge-like behaviour)",
       colour = NULL) +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **Overshoot zones:** At which depth ranges do the higher-degree
    polynomials deviate most from the data (overshoot/oscillation)?\
    
Mostly at Depth > 3000.
    
-   **Edge stability:** How does the **GAM** behave at
    shallowest/deepest depths compared with polynomials of degree ≥ 4?\
    
It fits the data points considerably better.
    
-   **Parsimony vs realism:** If a degree-5 polynomial shows a slightly
    lower AIC than degree-3, would you still prefer the GAM? Why?\
    
I would still prefer the GAM because high degree polynomials can overfit the data or introduce the Runge effect, and thus be harder to generalize from.
    
-   **Prediction risk:** What practical risks arise if stakeholders act
    on edge-unstable predictions (e.g., setting thresholds at range
    limits)?
    
They might act opposite to how they would if the prediction was not edge-unstable.
:::

## Part C — How GAMs work (inner workings, lightly) (10 min)

::: {.alert .alert-info}
**Task C1 (concept).**

In `mgcv`, $f(x)$ is built from spline **basis functions** with a
smoothness penalty ($\lambda$).

Optimal $\lambda$ is estimated by REML or GCV; larger
$\lambda \Rightarrow$ smoother curve. **edf** (in `summary()`) reflects
the effective flexibility.
:::

::: {.alert .alert-info}
**Task C2 (hands-on) — Vary the basis dimension `k` to confirm
adequacy**

**What you’ll do (and why):**

-   **Purpose:** Check whether the chosen spline **basis size** (`k`) is
    large enough to capture the signal without imposing an artificial
    ceiling on model flexibility.\
-   **How it works:** In `mgcv`, `k` sets the **maximum** complexity of
    the smooth; the model then estimates the **effective degrees of
    freedom (edf)** via REML/GCV. Typically, `edf ≤ k - 1` for cubic
    regression splines.\
-   **Underspecification risk:** If `k` is too small, the smoother may
    be **forced** to be too simple even if the penalty would allow more
    wiggle—this leads to biased fits.\
-   **Diagnostics:** Use `gam.check()`’s **k-index** test. Warnings (low
    k-index and small p-values) suggest increasing `k` and refitting. If
    increasing `k` does **not** change the fitted curve or edf
    materially, the smaller `k` is sufficient.\
-   **Practical tip:** Start modest (e.g., `k=10`), increase to a larger
    value (e.g., `k=30`), compare `edf`, AIC, and residual diagnostics.
    With REML, a **too-large** `k` is usually safe (penalty controls
    overfitting), but avoid excessively huge `k` for computation and
    concurvity stability.
:::

```{r}
#| label: C2-k-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Compare k=10 vs k=30, inspect edf/AIC, and check k-index in gam.check()

# 1) Fit two GAMs with different basis dimensions
m_k10 <- gam(Sources ~ s(Depth, bs = "cr", k = 10), data = dat, method = "REML")
m_k30 <- gam(Sources ~ s(Depth, bs = "cr", k = 30), data = dat, method = "REML")

# 2) Compare model summaries (edf, R^2, etc.) at a glance
bind_rows(
  broom::glance(m_k10) %>% dplyr::mutate(model = "k=10"),
  broom::glance(m_k30) %>% dplyr::mutate(model = "k=30")
) %>%
  dplyr::select(model, df = df.residual, AIC, deviance, r.squared = adj.r.squared)

# 3) Diagnostics: residual patterns & basis adequacy (k-index)
par(mfrow = c(2, 2))
gam.check(m_k10)
par(mfrow = c(2, 2))
gam.check(m_k30)
par(mfrow = c(1, 1))

# Optional (for visual comparison): overlay fitted curves
# newd <- tibble(Depth = seq(min(___$___), max(___$___), length.out = 400))
# pred10 <- dplyr::bind_cols(newd, as_tibble(predict(m_k10, newd, se.fit = TRUE, type = "response")))
# pred30 <- dplyr::bind_cols(newd, as_tibble(predict(m_k30, newd, se.fit = TRUE, type = "response")))
# ggplot(dat, aes(Depth, Sources)) +
#   geom_point(alpha = 0.5) +
#   geom_line(data = pred10, aes(y = fit), linewidth = 0.9, linetype = 2) +
#   geom_line(data = pred30, aes(y = fit), linewidth = 1)

```

::: {.alert .alert-success}
**Questions**

-   **edf stability:** Do `edf` and the fitted curve change meaningfully
    when moving from `k=10` to `k=30`? What does that imply?\
    
The edf changes from 8.52 to 12.9. It implies that the gam is not a lot more wiggly when increasing the k to 30.
    
-   **k-index check:** Does `gam.check()` flag basis inadequacy (k-index
    test)? If yes, what new `k` would you try next, and why?\
    
No, there is no basis inadequacy. P-value is 0.99.
    
-   **Parsimony vs safety:** Given similar fits, which `k` would you
    keep for subsequent analyses, and what’s your rationale?\
    
Given that the AIC is much lower for k=30, I would go with this model, as it fits the data better.
    
-   **Computational trade-offs:** When might an unnecessarily large `k`
    become problematic, even if REML penalisation keeps the curve
    smooth?
    
When you have a very large dataset with many observations.
:::

## Part D — GAMs vs GLMs (what changes?) (10 min)

::: {.alert .alert-info}
**Task D1 — Fit a GLM (Gaussian identity) and compare with the GAM**

**What you’ll do (and why):**

-   **Baseline model:** Fit a **GLM with Gaussian errors and identity
    link**: `Sources ~ Depth`. This is the linear benchmark against
    which we compare the GAM.\
-   **Model comparison:** Use **AIC** to compare `m_glm` vs `m_gam`.
    Lower AIC indicates a better trade-off between fit and complexity.\
-   **Diagnostics:** Inspect residuals of **both** models. Even with
    Gaussian errors, a linear mean structure can be misspecified if the
    *shape* is non-linear; GAMs can capture this via a smooth
    `s(Depth)`.\
-   **Takeaway:** A GAM can outperform a linear GLM under Gaussian
    errors when the **systematic component** (mean–covariate relation)
    is curved.

**Practical tips:** - A lower AIC for the GAM alongside cleaner
residuals supports non-linearity in the mean.\
- If AICs are close, prioritise **diagnostics** and **interpretability**
(edge behaviour, residual structure).
:::

```{r}
#| label: D1-glm-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit a Gaussian-identity GLM and compare with the GAM via AIC; inspect residuals.

# 1) Fit the baseline GLM (linear mean)
m_glm <- glm(Sources ~ Depth, data = dat, family = gaussian(link="identity"))

# 2) Compare to GAM via AIC
AIC(m_k30, m_glm)

# 3) Quick residual diagnostics (GLM): residuals–fitted and QQ
par(mfrow = c(1, 2))
plot(residuals(m_glm), fitted(m_glm), main = "GLM: Residuals vs Fitted",
     xlab = "Fitted", ylab = "Residuals"); abline(h = 0, lty = 2)
qqnorm(dat$Sources, main = "GLM: Normal Q-Q"); qqline(dat$Sources)
par(mfrow = c(1, 1))

# 4) (Optional) Re-run GAM diagnostics here for side-by-side judgement
# par(mfrow = c(2, 2)); gam.check(___); par(mfrow = c(1, 1))
```

:: {.alert .alert-success} **Questions**

-   **AIC verdict:** Which model has the lower AIC? Is the difference
    large enough to be compelling (\> \~2–4)?\
    
The gam model has a much lower AIC (by >100).
    
-   **Residual patterns:** Do you see curvature in **GLM** residuals vs
    fitted values that disappears in the **GAM**?\
    
Yes, the curvature is minimized a bit, though there still isn't normality of residuals of the gam.
    
-   **Model misspecification:** Why can a linear GLM be inadequate even
    if Gaussian error assumptions roughly hold?\
    
Because the true relationship may be non-linear (another assumption of linear GLM violated), so a straight line would systematically misrepresent the trend even if the errors are gaussian.
    
-   **Communication:** How would you explain to stakeholders why a
    smooth (GAM) is preferable here over a straight line?
    
I would say that the smooth follows the shape of the data without forcing it to be straight, so it captures important bends and avoids misleading over-simplification.
    
    :::

## Part E — Model fit & selection (15 min)

::: {.alert .alert-info}
**Task E1 — Compare alternative GAMs: different bases and smoothness
criteria**

**What you’ll do (and why):**

-   **Purpose:** To explore how GAM fits depend on the **choice of
    spline basis** and the **method for smoothing parameter
    estimation**.\
-   **Spline bases compared:**
    -   `"cr"` = **cubic regression spline** — efficient and common
        default.\
    -   `"tp"` = **thin plate regression spline** — more flexible,
        recommended when smoothness complexity is unknown.\
    -   `"cs"` = **cubic regression spline with shrinkage** — like
        `"cr"`, but allows the smoother to shrink towards 0 if
        unsupported by data (useful in model selection).\
-   **Smoothing selection methods:**
    -   **REML (Restricted Maximum Likelihood):** generally more stable
        and conservative.\
    -   **GCV.Cp (Generalised Cross Validation):** can be faster but
        sometimes under-smooths.\
-   **Approach:** Fit three models with the same data but different
    combinations of basis and smoothing method, then compare them using
    **AIC**.

**Key message:** If the AIC values and fitted curves are similar, the
choice of basis/method is less critical. Differences may matter in small
samples or when doing **variable selection**.
:::

```{r}
#| label: E1-compare-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Compare GAMs with different spline bases and smoothing selection criteria

# 1) Fit GAMs with different bases and methods
# Fit three GAMs with different bases/methods
# Cubic regression spline with REML smoothing parameter selection
m_cr_reml <- gam(Sources ~ s(Depth, bs = "cr", k = 30), data = dat, method = "REML")
# Thin plate regression spline with GCV.Cp smoothing parameter selection  
m_tp_gcv  <- gam(Sources ~ s(Depth, bs = "tp",k = 30), data = dat, method = "GCV.Cp")
# Cubic regression spline with shrinkage and REML smoothing parameter selection
m_cs_reml <- gam(Sources ~ s(Depth, bs = "cs", k = 30), data = dat, method = "REML")   # cubic shrinkage

# 2) Compare the models using AIC
AIC(m_cr_reml, m_tp_gcv, m_cs_reml)

```

::: {.alert .alert-info}
**Task E2 — Diagnostics and concurvity**

**What you’ll do (and why):**

-   **Residual diagnostics:** Use `gam.check()` to assess whether the
    model assumptions hold. This produces:
    -   Residuals vs fitted values (should show no clear pattern).\
    -   Q-Q plot of residuals (should be close to the line if residuals
        are Gaussian).\
    -   Scale–location plot (homogeneity of variance).\
    -   Histogram of residuals.\
    -   **k-index test** for basis dimension adequacy (low k-index
        suggests `k` may be too small).\
-   **Concurvity diagnostics:** Use `concurvity()` to check for strong
    correlations between smooth terms.
    -   Concurvity is like multicollinearity but for smooths.\
    -   Values close to 1 indicate redundancy between smooths, which can
        make interpretation unstable.\
    -   With only one smooth term (as in this example), concurvity isn’t
        a problem but running it introduces the concept for when you add
        more predictors.

**Key message:** A GAM is only trustworthy if residual diagnostics look
reasonable and concurvity is low. Always validate your model fit before
interpreting smooths.

**Decision prompt.** Pick a “best” model using AIC + diagnostics +
interpretability. State your choice and why (method, edf, residuals).
:::

```{r}
##| label: E2-diag-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Run model diagnostics (gam.check) and assess concurvity

# 1) Residual diagnostics for the chosen model
gam.check(m_cs_reml)

# 2) Concurvity assessment (more relevant with multiple smooths)
concurvity(m_cs_reml, full = TRUE)

# (add-on) — Visual residuals–fitted overlay with ggplot2**
## **What you’ll do (and why):**
## - Create a tidy data frame of **fitted values** (on the response scale) and **Pearson residuals** from your chosen GAM (e.g. `m_cr_reml`).
## - Plot **residuals vs fitted** using `ggplot2`, add a **zero reference line**, and a light **LOESS trend** to spot structure.
## - This complements `gam.check()` with a modern diagnostic visual and makes it easy to layer aesthetics if needed (e.g., colour by a factor).
# Goal: Build a ggplot residuals–fitted overlay for your chosen GAM (e.g., m_cr_reml)
# 1) Create a small diagnostics tibble with fitted values and Pearson residuals
#diag_df <- tibble(
#  fitted = fitted(___, type = "___"),
#  resid  = residuals(___, type = "___")
#)

# 2) Plot residuals vs fitted with a LOESS guide
#ggplot(diag_df, aes(x = ___, y = ___)) +
#  geom_hline(yintercept = 0, linetype = 2) +
#  geom_point(alpha = ___) +
#  geom_smooth(method = "loess", se = FALSE, span = 0.8) +
#  labs(title = "Residuals vs Fitted (GAM)",
#       x = "Fitted values",
#       y = "Pearson residuals") +
#  theme_minimal()

```

::: {.alert .alert-success}
**Questions**

-   **Residuals:** Do the residual plots suggest non-linearity,
    heteroscedasticity, or non-normality?\
    
The qq plot shows some deviation in the tails and around -2 and 2, suggesting non-normality.

The residuals vs fitted plot shows no clear curvature, but variance increases slightly with fitted values (mild heteroscedasticity).

Histogram is roughly centred. Overall: mostly OK, but not perfectly gaussian.
    
-   **k-index:** Does `gam.check()` suggest that the basis size `k` was
    adequate? If not, what would you try next?\
    
Reports k-index = 1.33 with p = 0.98, so basis dimension is adequate.

If the k-index had been < 1 and p small, I would try increasing k or using a different smoother.
    
-   **Concurvity:** Why is concurvity not an issue here, but critical
    when you have multiple smooth terms?\
    
With only one smooth term, concurvity is essentially zero and irrelevant.

Concurvity matters when multiple smooths are included: it measures how much one smooth can be approximated by others.
    
-   **Practice:** How would high concurvity affect your interpretation
    of a GAM with two correlated covariates?\

High concurvity between two smooth terms would make it difficult to separate their individual effects.

The fitted curves would be unstable and interpretation of each predictor’s influence would be unreliable, even if overall fit looks good.
:::

## Part F — Beyond a single station: Station effect + one smooth (15 min)

::: {.alert .alert-info}
**Task F1 — Two-station example with a shared smooth and a station
shift**

**What you’ll do (and why):**

-   **Purpose:** Combine Stations **8** and **13** and model a **common
    depth effect** via a single smooth `s(Depth)` while allowing a
    **parametric shift** between stations (an intercept difference via
    `+ Station`).\
-   **Depth overlap:** Restrict both stations to the **shared depth
    range** so that the common smooth isn’t biased by non-overlapping
    extremes (a classic source of spurious differences).\
-   **Model:** `Sources ~ s(Depth, bs="cr") + Station` with **REML**
    smoothing selection.\
-   **Inference:** Use `summary()` and `anova()` to assess:
    -   the **shape and significance** of `s(Depth)` (edf, F, p)\
    -   whether **Station** has a significant **offset** (parametric
        coefficient)

**Key idea:** This tests whether stations differ mainly by a **level
shift** while **sharing the same curve shape** with depth. If the offset
is insufficient (i.e., shapes appear different), move to **by-smooths**
in Task F2 (e.g., `s(Depth, by = Station) + Station`).
:::

```{r}
#| label: F1-two-stations-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit a GAM with a shared smooth over Depth and a parametric Station shift

# 1) Subset Stations 8 and 13; keep key variables
d2 <- ISIT |>
  dplyr::filter(Station %in% c(8, 13)) |>
  dplyr::transmute(Station = factor(Station),
                   Depth   = SampleDepth,
                   Sources = Sources)

# 2) Restrict to shared Depth overlap
rng <- d2 |>
  dplyr::group_by(Station) |>
  dplyr::summarise(mn = min(Depth), mx = max(Depth), .groups = "drop")
lo  <- max(rng$mn); hi <- min(rng$mx)
d2o <- d2 |>
  dplyr::filter(Depth > lo, Depth < hi)

# 3) Fit shared-smooth + station-offset model
m_two <- mgcv::gam(Sources ~ s(Depth, bs = "cr") + Station, data = d2o, method = "REML")

# 4) Inspect results
summary(m_two)
anova(m_two)

```

::: {.alert .alert-success}
**Questions**

-   **Offset vs shape:** Does the station effect look like a **constant
    shift** across depths, or do you suspect **shape differences**?\
    
The significant Station 13 coefficient (–12.3) suggests a constant downward shift relative to Station 8.

Because we only included one smooth, the model assumes the same curve shape for both.
    
-   **Shared range:** How might failing to restrict to the **common
    depth range** distort conclusions?\
    
If you don’t restrict to the common depth overlap, one station’s curve might be extrapolated outside its observed range.

That could make it look like stations differ in shape, when the difference is actually due to extrapolation.
    
-   **Evidence:** What do the **edf** and p-values for `s(Depth)`
    suggest about non-linearity?\
    
The smooth of Depth has edf ≈ 4.2 and a p < 0.05.

That means there is strong evidence for non-linearity: the effect is not just a straight line, but a flexible curve.
    
-   **Next step:** If the station offset is significant but residual
    plots show structure by station, what model term would you add next
    (and why)?
    
Add a by-smooth term (s(Depth, by = Station)) so each station can have its own smooth curve.

This allows both the overall offset and potential shape differences to be modeled.
:::

::: {.alert .alert-info}
**Task F2 — Plot the common smooth with a station offset**

**What you’ll do (and why):** - **Purpose:** Visualise the fitted
**shared smooth over Depth** with a **station-specific intercept shift**
from **Task F1**. This lets you see whether a single curve shape plus a
vertical offset captures both stations adequately.\
- **Prediction grid:** Build a tidy grid over the **common depth range**
(`d2o`) for **each station** so the offset appears as parallel curves
with identical shapes.\
- **Uncertainty:** Use `predict(..., se.fit = TRUE, type = "response")`
to obtain fitted values and pointwise SEs on the **response scale**.
Plot ribbons as \~95% intervals (`fit ± 2*se.fit`).\
- **Visual cues:** Scatter the raw data by station, overlay the
station-wise fitted lines, and add ribbons to compare fit quality across
stations and depths.

**Key idea:** If the curves appear **parallel** (same shape, vertical
shift), the shared-smooth + station-offset model is appropriate. If
shapes visibly diverge, consider **by-smooths**
(`s(Depth, by = Station) + Station`).
:::

```{r}
#| label: F2-plot-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Plot the common smooth + station offset with uncertainty bands

# 1) Build a prediction grid over the common depth range for each station
new2 <- tidyr::expand_grid(
  Depth   = seq(min(d2o$Depth), max(d2o$Depth), length.out = 500),
  Station = levels(d2o$Station)
)

# 2) Predict on the response scale with standard errors
p2 <- dplyr::bind_cols(
  new2,
  as_tibble(predict(m_two, new2, se.fit = TRUE, type = "response"))
)

# 3) Plot raw data + fitted lines + ribbons
ggplot(d2o, aes(Depth, Sources, colour = Station)) +
  geom_point(alpha = 0.7) +
  geom_line(data = p2, aes(y = fit)) +
  geom_ribbon(data = p2,
              aes(y = fit, 
                  ymin = fit - 2*se.fit,
                  ymax = fit + 2*se.fit,
                  fill = Station),
              alpha = 0.15, colour = NA) +
  labs(title = "GAM with shared smooth and station shift",
       subtitle = "Model: s(Depth) + Station") +
  theme_minimal() +
  guides(fill = "none")
```

::: {.alert .alert-success}
**Questions**

-   **Parallelism:** Do the two fitted lines look **parallel** across
    the depth range? If not, where do they diverge most?\
    
No, the lines are not parallel. They diverge most around depths of 1500–2000.
    
-   **Uncertainty:** Where are the ribbons widest? Is that due to sparse
    data or genuine variability?\
    
Ribbons are widest near 1000–1500 (Station 8) and 2000–2500 (Station 13), and at extremes for both, likely due to sparse data in those ranges.
    
-   **Fit adequacy:** Are there systematic zones where one station’s
    data consistently sits above/below the fitted line?\
    
Yes, systematic deviations exist, e.g., Station 8 points are above the fit at 500–1000, while Station 13 points are below in the same range.
    
-   **Next model:** If shapes differ, how would **by-smooths** change
    the specification and interpretation?
    
Use by-smooths (s(Depth, by = Station)) to allow each station its own smooth shape instead of a shared one.
:::

## Part G — Expanding GAMs: Beyond Gaussian (10 min)

::: {.alert .alert-info}
**Task G1 — Count responses: Poisson vs Negative Binomial GAMs +
overdispersion check**

**What you’ll do (and why):** - **Motivation:** `Sources` are
non-negative counts. Gaussian GAMs can fit the mean curve but ignore
count-like mean–variance structure.\
- **Families:**\
- **Poisson**: $\mathbb{E}[Y]=\mu$, $\mathrm{Var}(Y)=\mu$
(equidispersion).\
- **Negative Binomial (NB)**: $\mathrm{Var}(Y)=\mu + \mu^2/\theta$
(extra-Poisson/overdispersion handled by $\theta$).\
- **Approach:** Fit `Poisson` and `NB` GAMs with the same smoother as
before, compare **AIC**, and check **overdispersion** (Poisson).\
- **Overdispersion rule of thumb:** If
$\text{ratio} = \frac{\text{residual deviance}}{\text{df residual}} \gtrsim 1.5\!-\!2$,
Poisson is likely too restrictive → prefer **NB**.\
- **Outcome:** You’ll decide whether a count family yields better
fit/diagnostics than the Gaussian GAM.

**Notes:**\
- `type="response"` predictions keep the plot in data units.\
- With counts and small means, consider using **Pearson** residuals for
checks.
:::

```{r}
# Round the Sources to force a count
#| label: G1-counts-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit Poisson and NB GAMs for count data, compare AIC, and check overdispersion


# Round the Sources to force a count
dat$rnd.Sources <- round(dat$Sources)
# 1) Fit count-family GAMs with the same smoother used before
m_pois <- mgcv::gam(rnd.Sources ~ s(Depth, bs = "cr", k = 30), data = dat, family = poisson(), method = "REML")
m_nb   <- mgcv::gam(rnd.Sources ~ s(Depth, bs = "cr", k = 30), data = dat, family = nb(),    method = "REML")
summary(m_pois)
summary(m_nb)

# 2) Compare AICs (Gaussian vs Poisson vs NB)
AIC(m_k30, m_pois, m_nb)

# 3) Quick Poisson overdispersion checks
# (a) Residual deviance / df
with(broom::glance(m_pois), deviance / df.residual)

# (b) Pearson chi-square / df (often more informative)
chi_phi <- sum(residuals(m_pois, type = "pearson")^2) / df.residual(m_pois)
chi_phi

# OPTIONAL: Visual residuals–fitted overlay for NB model
# diag_nb <- tibble(fitted = fitted(___, type = "response"),
#                   resid  = residuals(___, type = "pearson"))
# ggplot(diag_nb, aes(fitted, resid)) +
#   geom_hline(yintercept = 0, linetype = 2) +
#   geom_point(alpha = 0.6) +
#   geom_smooth(method = "loess", se = FALSE, span = 0.8) +
#   labs(title = "Residuals vs Fitted (NB GAM)",
#        x = "Fitted (response scale)", y = "Pearson residuals") +
#   theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **AIC & diagnostics:** Do Poisson/NB improve AIC versus the Gaussian
    GAM? What happens to residual patterns?\
    
Poisson has the lowest AIC (m_pois AIC = 213.18) vs NB (215.18) vs Gaussian (243.30).

So Poisson fits better by AIC; NB is a little worse than Poisson and both better than Gaussian.

Diagnostics (from summaries) show very high deviance explained (~96.5%) and a very wiggly but highly significant smooth (edf ≈ 5.88, p < 2e-16). The dispersion metrics are < 1, so overdispersion is not an issue.
    
-   **Dispersion:** What is the Poisson dispersion ratio? If high, how
    does NB change the picture?\
    
Poisson dispersion ratio (residual deviance / df) = 0.696; Pearson chi-square / df = 0.573. Both < 1 → underdispersion (variance < mean).

The fact that the dispertion ratio is low, reduces NB to something very close to Poisson here. So NB does not meaningfully change dispersion in this case.
    
-   **Interpretation:** How do you explain (to non-statisticians) the
    practical difference between **GAM (Gaussian)** and **GAM
    (Poisson/NB)** for these data?\
    
GAM (Gaussian): treats the response like a continuous quantity with constant-variance errors — okay for smooth mean trends.

GAM (Poisson/NB): treats the response as counts. Practically: Poisson/NB give predictions that respect counts and often fit count data better.

In your results, the Poisson count-gam fits much better (AIC) than the gaussian gam. The NB doesn’t add much here, so Poisson is the preferred count model.
    
-   **Next steps:** If NB still shows structure, what else might you try
    (offsets, additional covariates, alternative links, zero-inflation,
    etc.)?
    
Check the data-generation/rounding step: rounding continuous Sources to integers can artificially change variance.

If residual structure remains: try adding relevant covariates or interactions (e.g., other environmental predictors, station, seasonal terms), or allow different smooths by group (by=) if groups behave differently.

Also check link/specification and zero structure: if many zeros -> zero-inflated / hurdle models.

Finally, inspect residual vs fitted to guide whether model structure or missing covariates are the issue.
    
:::

## Summary of the Practical

This practical introduced students to the use of Generalised Additive
Models (GAMs) through the bioluminescence depth dataset. We began by
visualising raw data at single stations, highlighting the inadequacy of
linear fits and motivating the need for flexible smoothers. By fitting
cubic regression splines in mgcv::gam, students explored the principle
of replacing fixed parametric terms with smooth functions to capture
non-linear relationships.

We contrasted GAMs with polynomial regressions, which can mimic
non-linearity but suffer from instability at range edges (Runge’s
phenomenon). This emphasised why GAMs are preferable: they avoid
oscillations, allow local flexibility, and are regularised through
penalisation. The role of the basis dimension (k) and smoothing
parameter (λ) was examined, with diagnostic tools (gam.check, residual
plots, k-index) showing how to assess adequacy and avoid
under/over-smoothing.

We extended the models by comparing different spline bases (cubic
regression, thin-plate, shrinkage) and smoothness criteria (REML, GCV).
Students practised evaluating model fit with AIC and concurvity checks,
reinforcing that GAMs retain the statistical rigour of GLMs while
offering flexibility. A comparison between GLMs and GAMs (Gaussian
identity) demonstrated why GAMs often outperform when relationships are
non-linear, even with the same error family.

Building complexity, we modelled multiple stations with shared smooths
and parametric shifts, and discussed when by-smooths are preferable.
Finally, the practical illustrated GAMs beyond Gaussian responses,
fitting Poisson and Negative Binomial GAMs to count data, and
highlighting the importance of overdispersion checks.

Overall, the session showed that GAMs form a natural extension of
regression models:

-   From a single smoother (non-linear regression replacement),
-   Through diagnostics and model selection,
-   To flexible multi-station and non-Gaussian applications.

::: callout-note
**Key Take-Home Points** \* GAMs extend GLMs by allowing smooth,
data-driven functional forms for predictors. \* Polynomials $\neq$
smoothers: GAMs avoid Runge’s phenomenon and handle local variation
better. \* Diagnostics matter: always check gam.check, AIC, and
concurvity before trusting results. \* Flexible families: GAMs work with
Gaussian, Poisson, binomial, NB, and beyond. \* Interpretation: Smooth
terms represent shapes, not single coefficients — describe curves, not
slopes.
:::
