---
title: "Week 6 Practical – Tree Based Models"
subtitle: "Using CART, Random Forests, and Boosted Regression trees"
author: "Student Name: René Gier Vemmelund"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
```

## Setup

**Packages used**: Base R for processing; `rpart`, `randomForest`, `gbm` for modelling; `ggplot2` for plots.

```{r setup2, message=FALSE, warning=FALSE}
# Load required packages
#install.packages('gbm')
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ggplot2)
theme_set(theme_bw(base_size = 12))

# Set seed for reproducible results
set.seed(123)


```

## About the Data

The objective is to predict deciduous broadleaf forests (TBMF) from climate variables, then map predictions globally. The dataset includes three files:

- `BiomeData.csv` – Long-format table of sampled sites (≈500 points per vegetation type) with a unique ID and a biome label.
- `ClimateData.csv` – Site-level climate variables with matching IDs.
- `WorldClimateData.csv` – Global grid of climate variables for prediction (includes longitude, latitude, and the same predictor column names).

---

## Task 1 – Building a Presence–Absence Matrix for TBMF (10–12 min)

**Objective:**

The first task involves converting the long-format `BiomeData.csv` (containing site `ID` and `Biome` columns) into a **wide presence–absence** format for the focal biome (**Temperate Broadleaf & Mixed Forests**, abbreviated as TBMF). The goal is to create a binary response variable (1 = present, 0 = absent) for each unique location `ID`.


```{r}
# ============================================================================
# STEP 1: LOAD DATA FILES
# ============================================================================

# Read biome dataset
# HINT: stringsAsFactors keeps text as characters, [,-1] removes first column
biome <- read.csv("DATA/BiomeData.csv", 
                  stringsAsFactors = FALSE)[,-1]  # Remove column 1

# Read climate dataset
clim  <- read.csv("DATA/ClimateData.csv", 
                  stringsAsFactors = FALSE)[,-1]

# ============================================================================
# STEP 2: CREATE PRESENCE-ABSENCE DATA FOR TBMF
# ============================================================================

# Extract all unique location IDs
unique_ids <- unique(biome$ID)  # Get unique values from biome$ID

# Create presence-absence indicator for TBMF
# HINT: tapply() groups data and applies a function to each group
# HINT: any() returns TRUE if at least one value is TRUE
DBF_flag <- tapply(
  # Check if biome name matches TBMF (creates TRUE/FALSE)
  biome$BIOME.Name == "Temperate Broadleaf & Mixed Forests",  # Complete the biome name
  
  # Group by location ID
  biome$ID,  # Which column contains the ID?
  
  # Function: Check if ANY row has TBMF, convert to integer (1/0)
  function(x) as.integer(any(x))  # Use any() function
)

# Convert results to a data frame
dbf_df <- data.frame(
  ID = as.integer(names(DBF_flag)),    # Extract IDs from names
  TBMF = as.integer(DBF_flag),         # Presence/absence values
  row.names = NULL
)

# ============================================================================
# STEP 3: VERIFY THE RESULTS
# ============================================================================

# Display frequency table
# HINT: table() counts occurrences of each value
table(dbf_df$TBMF)  # Count TBMF values (0 and 1)
```

**Reflection Questions:**

1. If some sites contain multiple biomes, why is `any()` a reasonable function for aggregating to presence–absence? *The biome should should at least be present at the site. any() checks if at least one of the values are true?*
2. What challenges arise if the positive class ("Temperate Broadleaf & Mixed Forests") is very rare or very common in the dataset? *Decision trees have the risk of prioritizing the purity of the class with highest frequency. So e.g., if the positive class is rare the model will be worse at predicting the presence of it.*
3. Why is a site-level binary response preferable to a multi-class target for this exercise? *Our goal is to predict the presence of TBMF, not which biome is at each site.*

---

## Task 2 – Matching Climate Data to Sites (8–10 min)

**Objective:**

This task merges the site-level TBMF response variable with corresponding climate predictors using base R's `merge()` function. Rows with missing climate values are removed to ensure complete datasets for modeling.

```{r}
# ============================================================================
# STEP 1: MERGE BIOME AND CLIMATE DATA
# ============================================================================

# Combine TBMF presence/absence with climate measurements
# HINT: merge() combines data frames by a common column
dat <- merge(
  dbf_df,              # TBMF data frame (dbf_df)
  clim,              # Climate data frame (clim)
  by = "ID",       # Column name to match on (ID)
  all.x = TRUE       # Keep all rows from first data frame? (TRUE/FALSE)
)

# ============================================================================
# STEP 2: REMOVE ROWS WITH MISSING CLIMATE DATA
# ============================================================================

# Identify climate predictor columns
# HINT: setdiff() finds elements in first set but not in second
pred_cols <- setdiff(
  colnames(dat),           # All column names from dat
  c("ID", "TBMF")          # Columns to exclude (ID and TBMF)
)

# Keep only rows with complete climate data
# HINT: complete.cases() returns TRUE for rows with no missing values
dat <- dat[
  complete.cases(dat[, pred_cols, drop = FALSE]),  # Check predictor columns
]

# Display dataset structure
str(dat)  # Show structure of dat

# ============================================================================
# STEP 3: CHECK CLASS BALANCE
# ============================================================================

# Calculate proportion of absent (0) vs present (1)
# HINT: prop.table() converts counts to proportions
prop.table(table(dat$TBMF))  # Create table of TBMF column
```

**Reflection Questions:**

1. When removing locations with incomplete climate data, what types of locations might be systematically excluded, and how could this bias the understanding of TBMF distribution? *It might exclude remote areas and places in developing countries: Places where TBMF are less likely to be. So it could make TBMF seem more common.*
2. Why is class balance important for evaluating model accuracy? *It could introduce bias in predicting majority classes.*
3. Which climate variables might be collinear, and why do tree-based models typically handle collinearity well? *Mean and extremes might be collinear. Both for temperature and precipitation. Elevation is most likely also collinear with temperatures -> higher elev = colder. Tree-based models handle typically handle this well, because each decision at nodes is chosen to best split the data. Collinear predictors might introduce redundancy in the trees, but this can be overcome with pruning. It can however make it harder to conclude predictor importance.*

---

## Task 3a – CART: Fitting and Pruning (10 min)

**Objective:**

A decision tree model is built to predict TBMF presence/absence based on climate variables. Cross-validation is used to avoid overfitting, the tree is pruned to optimal size, decision rules are visualized, and the most important climate variables are identified.

```{r}
# ============================================================================
# STEP 1: BUILD THE INITIAL CLASSIFICATION TREE
# ============================================================================

# Create model formula: TBMF ~ all climate variables
# HINT: paste() with collapse joins elements with a separator
form <- as.formula(
  paste(
    "TBMF ~",                            # Response variable name
    paste(pred_cols, collapse = " + ")      # Join predictors with " + "
  )
)

# Fit a Classification And Regression Tree
# HINT: method = "class" for classification, cp = complexity parameter
cart0 <- rpart(
  form,                      # Formula (form)
  data = dat,               # Dataset (dat)
  method = "class",           # "class" for classification
  xval = 10,               # Number of cross-validation folds (10)
  control = rpart.control(
    cp = 0.001               # Small value like 0.001 for large initial tree
  )
)

# ============================================================================
# STEP 2: FIND THE OPTIMAL TREE COMPLEXITY
# ============================================================================

# Display cross-validation results
cp_tbl <- printcp(cart0)  # Print CP table for cart0

# Find tree size with minimum CV error
# HINT: which.min() returns position of minimum value
best_row <- which.min(cp_tbl[, "xerror"])    # Find min "xerror"
cp_min   <- cp_tbl[best_row, "CP"]            # Get CP at best row
xerr_min <- cp_tbl[best_row, "xerror"]            # Get xerror at best row
xerr_se  <- cp_tbl[best_row, "xstd"]            # Get xstd at best row

# Apply 1-standard-error rule
# HINT: max() finds the largest value meeting the condition
cp_1se <- max(
  cp_tbl[
    cp_tbl[, "xerror"] <= (xerr_min + xerr_se),  # xerr_min + xerr_se
    "CP"
  ]
)

# ============================================================================
# STEP 3: PRUNE THE TREE TO OPTIMAL SIZE
# ============================================================================

# Remove unnecessary branches
# HINT: prune() simplifies tree using CP threshold
cart <- prune(
  cart0,          # Original tree (cart0)
  cp = cp_1se      # Optimal CP (cp_1se)
)

# ============================================================================
# STEP 4: VISUALIZE THE DECISION TREE
# ============================================================================

# Create graphical representation
# HINT: type, extra, under, fallen.leaves control appearance
rpart.plot(
  cart,                   # Pruned tree (cart)
  type = 2,            # 2 shows split variables
  extra = 104,           # 104 shows probabilities and percentages
  under = TRUE,           # TRUE places info under boxes
  fallen.leaves = TRUE    # TRUE aligns terminal nodes at bottom
)

# ============================================================================
# STEP 5: IDENTIFY MOST IMPORTANT CLIMATE PREDICTORS
# ============================================================================

# Extract variable importance scores
cart_vi <- data.frame(
  Variable = names(cart$variable.importance),      # From cart
  Importance = as.numeric(cart$variable.importance) # From cart
)

# Sort from most to least important
# HINT: order() with negative value sorts descending
cart_vi <- cart_vi[order(-cart_vi$Importance), ]  # Sort by -Importance

# Display top predictors
head(cart_vi, 10)  # Show first 6 or 10 rows
```

**Reflection Questions:**

1. Why does pruning usually improve out-of-sample performance even if training accuracy drops? *Pruning simplifies the decision tree, reducing overfitting to the training data. While this lowers training accuracy, it improves generalization, leading to better performance on unseen data.*
2. What does the `xerror` in the CP table represent, and how does it differ from external 10-fold CV? *xerror is the cross-validated error from internal pruning validation within the rpart function. External CV is performed independently and typically gives a more unbiased estimate of model performance.*
3. How can the 1-SE choice be justified when describing results?
*The 1-SE rule selects the simplest model whose error is within one standard error of the minimum. It balances accuracy and simplicity reducing the risk of overfitting.*

---

## Task 3b – CART: External 10-Fold Cross-Validation (10 min)

**Objective:**

External 10-fold cross-validation provides an honest assessment of how well the CART model predicts TBMF on completely new data.

**Performance metrics definitions:**

- **Accuracy**: Percentage of correct predictions (both present and absent)
- **Sensitivity**: Of locations truly having TBMF, percentage correctly identified
- **Specificity**: Of locations truly lacking TBMF, percentage correctly identified
- **Balanced Accuracy**: Average of sensitivity and specificity, giving equal weight to both classes


```{r}
# External 10-fold cross-validation
# HINT: make_folds() creates fold indices, k = number of folds
# Helper: 10-fold CV splitter
make_folds <- function(n, k = 10) {
  folds <- sample(rep(1:k, length.out = n))
  split(seq_len(n), folds)
}

folds <- make_folds(nrow(dat), k = 10)  # Number of rows in dat, 10 folds

# HINT: lapply() applies function to each fold
cart_cv <- do.call(rbind, lapply(folds, function(idx_test){
  # Split into training and test sets
  # HINT: -idx_test means "all rows except test indices"
  tr <- dat[-idx_test, ]  # Training: all except test indices
  te <- dat[idx_test, ]          # Test: only test indices
  
  # Train model on training set
  m  <- rpart(form, data = tr, method = "class", xval = 10, 
              control = rpart.control(cp = 0.001))
  
  # Predict on test set
  # HINT: predict() with type="prob" gives probabilities
  p  <- predict(m, newdata = te, type = "prob")[, "1"]
  
  # Store results
  data.frame(ID = te$ID, TBMF = te$TBMF, prob = p)
}))

# Helper: accuracy metrics from a confusion table (binary, positive = 1)
# INPUTS:
#   truth - actual values (0 or 1) from the dataset
#   prob  - predicted probabilities (0 to 1) from the model
#   thr   - threshold for converting probabilities to predictions (default 0.5)
# OUTPUTS:
#   data frame with 4 metrics: Accuracy, Sensitivity, Specificity, BalancedAccuracy
acc_metrics <- function(truth, prob, thr = 0.5) {
  # Convert probabilities to binary predictions using threshold
  # If probability >= threshold, predict 1 (present), otherwise predict 0 (absent)
  pred <- ifelse(prob >= thr, 1, 0)
  # ============================================================================
  # CALCULATE CONFUSION MATRIX COMPONENTS
  # ============================================================================
  # True Positives (TP): Correctly predicted as present
  # Both truth and prediction are 1
  TP <- sum(truth == 1 & pred == 1)
  # True Negatives (TN): Correctly predicted as absent
  # Both truth and prediction are 0
  TN <- sum(truth == 0 & pred == 0)
  # False Positives (FP): Incorrectly predicted as present
  # Truth is 0 but prediction is 1 (Type I error)
  FP <- sum(truth == 0 & pred == 1)
  # False Negatives (FN): Incorrectly predicted as absent
  # Truth is 1 but prediction is 0 (Type II error)
  FN <- sum(truth == 1 & pred == 0)
  # ============================================================================
  # CALCULATE PERFORMANCE METRICS
  # ============================================================================
  # ACCURACY: Proportion of all predictions that were correct
  # Formula: (TP + TN) / Total
  # Interpretation: Overall correctness across both classes
  acc <- (TP + TN) / (TP + TN + FP + FN)
  
  # SENSITIVITY (Recall, True Positive Rate):
  # Of all actual positives, what proportion did we correctly identify?
  # Formula: TP / (TP + FN)
  # Interpretation: How good at finding what's really there
  # ifelse() prevents division by zero if no actual positives exist
  sens <- ifelse((TP + FN) == 0, NA, TP / (TP + FN))
  # SPECIFICITY (True Negative Rate):
  # Of all actual negatives, what proportion did we correctly identify?
  # Formula: TN / (TN + FP)
  # Interpretation: How good at confirming what's really absent
  # ifelse() prevents division by zero if no actual negatives exist
  spec <- ifelse((TN + FP) == 0, NA, TN / (TN + FP))
  # BALANCED ACCURACY:
  # Average of sensitivity and specificity
  # Gives equal weight to both classes regardless of class imbalance
  # na.rm = TRUE handles cases where sensitivity or specificity is NA
  bal_acc <- mean(c(sens, spec), na.rm = TRUE)
  # ============================================================================
  # RETURN RESULTS
  # ============================================================================
  # Return all metrics as a single-row data frame
  data.frame(
    Accuracy = acc, 
    Sensitivity = sens, 
    Specificity = spec, 
    BalancedAccuracy = bal_acc
  )
}

# Calculate performance metrics
# HINT: acc_metrics() needs true values, predictions, threshold (set to 0.5)
cart_cv_metrics <- acc_metrics(cart_cv$TBMF, cart_cv$prob, thr = 0.5)
cart_cv_metrics
```

**Reflection Questions:**

1. Why is nested cross-validation necessary (both internal `xval=10` within `rpart` AND external CV)? What would go wrong with only one level? *Nested cross-validation separates model tuning (internal CV) from performance evaluation (external CV). Without nesting, performance estimates may be biased because the same data is used for both tuning and testing.*
2. How should the 0.5 probability threshold used in the `acc_metrics` function be chosen depending on whether the goal is conservation planning, ecological research, or climate change prediction? *In conservation planning, a lower threshold may be preferred to avoid missing potential TBMF forest areas. In ecological research, 0.5 or higher could be used for balanced evaluation. For climate change prediction, the threshold could be higher to be more certain of TBMF presence.*
3. Does random folding violate independence assumptions when nearby locations have similar climates and biomes (spatial autocorrelation)? *Random folding can violate independence due to spatial autocorrelation. Nearby samples may be too similar, inflating performance estimates.*

---

## Task 4 – Random Forest (18–20 min)

**Objective:**

A Random Forest model is built—an ensemble of many decision trees that "vote" on predictions. The key parameter (`mtry`) is optimized, performance is evaluated through cross-validation, and the most important climate predictors are identified.

```{r}
# ============================================================================
# STEP 1: OPTIMIZE THE mtry PARAMETER
# ============================================================================

# Count climate predictor variables
p <- length(pred_cols)  # Number of predictor columns

# Create grid of mtry values to test
# HINT: seq() creates sequence, pmax() ensures minimum of 1
mtry_grid <- round(seq(1, p, length.out = 10)) # From 1 to p, create 7 values minimum

# Test each mtry value
# HINT: sapply() applies function to each element
oob <- sapply(mtry_grid, function(m) {  # Loop through mtry_grid
  # Build Random Forest
  # HINT: ntree = number of trees, mtry = variables per split
  rf_tmp <- randomForest(
    x = dat[, pred_cols],            # Predictor columns
    y = as.factor(dat$TBMF),    # Response as factor to make 
    ntree = 500,               # Number of trees (500)
    mtry = m,                # Current mtry value (m)
    importance = TRUE           # Calculate importance? (TRUE)
  )
  # Extract OOB error
  # HINT: Access error rate matrix, last row, "OOB" column
  rf_tmp$err.rate[rf_tmp$ntree, "OOB"]
})

# Organize results
oob_df <- data.frame(mtry = mtry_grid, OOB_Error = oob)
oob_df

# Select mtry with lowest OOB error
best_mtry <- oob_df$mtry[which.min(oob_df$OOB_Error)]

# ============================================================================
# STEP 2: BUILD THE FINAL RANDOM FOREST MODEL
# ============================================================================

# Train Random Forest using optimal mtry
rf <- randomForest(
  x = dat[, pred_cols],            # Predictor columns
  y = as.factor(dat$TBMF),    # Response as factor to make
  ntree = 500,               # Number of trees (500)
  mtry = best_mtry,                # Optimal mtry
  importance = TRUE,          # TRUE
  keep.forest = TRUE          # TRUE to save for predictions
)

# ============================================================================
# STEP 3: EXTERNAL 10-FOLD CROSS-VALIDATION
# ============================================================================

rf_cv <- do.call(rbind, 
  lapply(folds, function(idx_test){  # Use folds from before
    tr <- dat[-idx_test, ]
    te <- dat[idx_test, ]
    
    # Train Random Forest
    m <- randomForest(
      x = tr[, pred_cols],
      y = as.factor(tr$TBMF),
      ntree = 500,
      mtry = best_mtry,              # Use best_mtry
      importance = TRUE
    )
    
    # Predict on test fold
    p <- predict(m, newdata = te[, pred_cols], type = "prob")[, "1"]
    
    data.frame(ID = te$ID, TBMF = te$TBMF, prob = p)
  })
)

# Calculate performance metrics
rf_cv_metrics <- acc_metrics(rf_cv$TBMF, rf_cv$prob, thr = 0.5)
rf_cv_metrics

# ============================================================================
# STEP 4: VARIABLE IMPORTANCE
# ============================================================================

# Extract importance scores
# HINT: importance() function extracts from rf object
rf_vi <- data.frame(
  Variable = rownames(importance(rf)),
  MeanDecreaseGini = importance(rf)[, "MeanDecreaseGini"],
  MeanDecreaseAccuracy = importance(rf)[, "MeanDecreaseAccuracy"]
)

# Sort by Gini importance
rf_vi <- rf_vi[order(-rf_vi$MeanDecreaseGini), ]  # Sort descending

# Display top 10
head(rf_vi, 10)
```


**Reflection Questions:**

1. Why can OOB error serve as a trustworthy proxy for cross-validation in Random Forests? *OOB error uses the data left out of each bootstrap sample to evaluate model performance. Since each tree is tested on unseen data, OOB error provides an unbiased estimate similar to external cross-validation.*
2. How does `mtry` influence the bias–variance trade-off? *Smaller mtry increases randomness, reducing correlation between trees and lowering variance, but may raise bias. A larger mtry reduces bias by allowing stronger splits, but can increase variance due to more similar trees, as they more predictors are sampled at each split.*
3. What does the optimal `mtry` value reveal about climate-biome relationship complexity? *The best mtry was 3 variables. This indicates that prediction of TBMF is ecologically relatively simple. This makes sense as these large biome classes on a global scale can be predicted from precipitation and temperature.*
4. For conservation planning, would students recommend the interpretable CART model or the more accurate but "black box" Random Forest? *I might favor the CART for its interpretability -> transparent decision-making. However, if prediction accuracy is critical for resource allocation, rf may be preferred.*

<!-- --- -->

<!-- ## Task 5 – Gradient Boosting Machine (18–20 min) -->

<!-- **Objective:** -->

<!-- A Gradient Boosting Machine (GBM) is built—an ensemble method where trees are built sequentially, with each new tree correcting errors from previous trees. -->

<!-- ```{r} -->
<!-- # ============================================================================ -->
<!-- # STEP 1: BUILD GBM WITH INTERNAL CROSS-VALIDATION -->
<!-- # ============================================================================ -->

<!-- # Fit Gradient Boosting Machine -->
<!-- # HINT: distribution="bernoulli" for binary outcome -->
<!-- gbm0 <- gbm( -->
<!--   formula = form,                   # Use form from before -->
<!--   data = dat,                      # Use dat -->
<!--   distribution = "bernoulli",            # "bernoulli" for binary -->
<!--   n.trees = 5000,                   # Max trees to build (5000) -->
<!--   interaction.depth = 3,         # Tree depth (3) -->
<!--   shrinkage = 0.01,                 # Learning rate (0.01) -->
<!--   n.minobsinnode = 10,            # Min observations per node (10) -->
<!--   bag.fraction = 0.7,              # Subsample fraction (0.7) -->
<!--   cv.folds = 10,                  # CV folds (10) -->
<!--   keep.data = TRUE,                 # Keep data? (TRUE) -->
<!--   verbose = FALSE,                    # Print progress? (FALSE) -->
<!--   n.cores = 1                         # Needs to specify. Otherwise won't run. -->
<!-- ) -->

<!-- # Find optimal number of trees -->
<!-- # HINT: gbm.perf() finds best iteration using CV -->
<!-- best_iter <- gbm.perf(gbm0, method = "cv", plot.it = FALSE) -->

<!-- # ============================================================================ -->
<!-- # STEP 2: EXTERNAL 10-FOLD CROSS-VALIDATION - THIS CAN TAKE SOME TIME -->
<!-- # ============================================================================ -->

<!-- gbm_cv <- do.call(rbind,  -->
<!--   lapply(folds, function(idx_test){  # Use folds -->
<!--     tr <- dat[-idx_test, ] -->
<!--     te <- dat[idx_test, ] -->

<!--     # Train GBM -->
<!--     m <- gbm( -->
<!--       formula = form, -->
<!--       data = tr, -->
<!--       distribution = "bernoulli", -->
<!--       n.trees = best_iter,             # Use best_iter -->
<!--       interaction.depth = 3, -->
<!--       shrinkage = 0.01, -->
<!--       n.minobsinnode = 10, -->
<!--       bag.fraction = 0.7, -->
<!--       keep.data = FALSE,           # FALSE to save memory -->
<!--       verbose = FALSE, -->
<!--       n.cores = 1 -->
<!--     ) -->

<!--     # Predict on test fold -->
<!--     # HINT: predict.gbm() needs n.trees and type="response" -->
<!--     p <- predict(m, newdata = te, n.trees = best_iter, type = "response") -->

<!--     data.frame(ID = te$ID, TBMF = te$TBMF, prob = p) -->
<!--   }) -->
<!-- ) -->

<!-- # Calculate performance metrics -->
<!-- gbm_cv_metrics <- acc_metrics(gbm_cv$TBMF, gbm_cv$prob, thr = 0.5) -->
<!-- gbm_cv_metrics -->

<!-- # ============================================================================ -->
<!-- # STEP 3: EXTRACT VARIABLE IMPORTANCE -->
<!-- # ============================================================================ -->

<!-- # Calculate variable importance -->
<!-- # HINT: summary() on gbm object gives importance -->
<!-- gbm_vi <- summary(gbm0, n.trees = best_iter, plotit = FALSE) -->

<!-- # Display top 10 -->
<!-- head(gbm_vi, 10) -->
<!-- ``` -->

<!-- **Reflection Questions:** -->

<!-- 1. How do `shrinkage` and `interaction.depth` affect overfitting risk? -->
<!-- 2. Why does `gbm.perf` select fewer trees than the maximum fitted? -->
<!-- 3. What does the optimal stopping point reveal about climate-TBMF relationship complexity? -->

<!-- --- -->

<!-- ## Task 6 – Comparing Variables and Accuracy (8–10 min) -->

<!-- **Objective:** -->

<!-- Two summary tables are created for direct comparison across all three models. -->

<!-- ```{r} -->
<!-- # ============================================================================ -->
<!-- # STEP 1: CREATE UNIFIED VARIABLE IMPORTANCE TABLE -->
<!-- # ============================================================================ -->

<!-- # Extract CART rankings -->
<!-- # HINT: rank() with negative value ranks descending -->
<!-- cart_vars <- data.frame( -->
<!--   Model = "___",                        # "CART" -->
<!--   Variable = ___$Variable,              # From cart_vi -->
<!--   Rank = rank(___$Importance, ties.method = "___")  # Use negative for descending -->
<!-- ) -->

<!-- # Extract RF rankings -->
<!-- rf_vars <- data.frame( -->
<!--   Model = "___",                        # "RF" -->
<!--   Variable = ___$___,                   # Variable column from rf_vi -->
<!--   Rank = rank(___$___, ties.method = "___")  # Use MeanDecreaseGini -->
<!-- ) -->

<!-- # Extract GBM rankings -->
<!-- gbm_vars <- data.frame( -->
<!--   Model = "___",                        # "GBM" -->
<!--   Variable = ___$___,                   # var column from gbm_vi -->
<!--   Rank = rank(___$___, ties.method = "___")  # Use rel.inf -->
<!-- ) -->

<!-- # Combine all tables -->
<!-- # HINT: rbind() stacks data frames vertically -->
<!-- vars_all <- rbind(___, ___, ___) -->

<!-- # Display first 15 rows -->
<!-- head(___, ___) -->

<!-- # ============================================================================ -->
<!-- # STEP 2: CREATE UNIFIED PERFORMANCE METRICS TABLE -->
<!-- # ============================================================================ -->

<!-- # Combine CV metrics from all models -->
<!-- acc_tab <- rbind( -->
<!--   data.frame(Model = "___", ___),       # CART (pruned), cart_cv_metrics -->
<!--   data.frame(Model = "___", ___),       # Random Forest, rf_cv_metrics -->
<!--   data.frame(Model = "___", ___)        # GBM, gbm_cv_metrics -->
<!-- ) -->

<!-- # Display comparison -->
<!-- acc_tab -->
<!-- ``` -->

<!-- **Reflection Questions:** -->

<!-- 1. Why might top-ranked variables differ between models? -->
<!-- 2. If two models have equal accuracy, which secondary metric should be prioritized? -->
<!-- 3. How might importance scales be harmonized to enable comparison? -->

<!-- --- -->

<!-- ## Task 7 – Global Prediction (10–12 min) -->

<!-- **Objective:** -->

<!-- The best-performing model is used to predict TBMF probability across the entire world. -->

<!-- ```{r} -->
<!-- # ============================================================================ -->
<!-- # STEP 1: LOAD GLOBAL CLIMATE GRID -->
<!-- # ============================================================================ -->

<!-- # Read worldwide climate data -->
<!-- world <- read.csv("___", stringsAsFactors = ___)[,___] -->

<!-- # Validate data compatibility -->
<!-- # HINT: stopifnot() stops if conditions are FALSE -->
<!-- stopifnot( -->
<!--   all(___ %in% names(___)),      # All pred_cols in world -->
<!--   all(c("___","___") %in% names(___))  # x and y coordinates in world -->
<!-- ) -->

<!-- # ============================================================================ -->
<!-- # STEP 2: AUTOMATICALLY SELECT BEST MODEL -->
<!-- # ============================================================================ -->

<!-- # Identify model with highest balanced accuracy -->
<!-- # HINT: which.max() finds position of maximum value -->
<!-- best_name <- ___$Model[which.max(___$___)]  # Find max BalancedAccuracy -->
<!-- best_name -->

<!-- # ============================================================================ -->
<!-- # STEP 3: GENERATE GLOBAL PREDICTIONS -->
<!-- # ============================================================================ -->

<!-- # Apply winning model -->
<!-- # HINT: grepl() checks if pattern matches text -->
<!-- if (grepl("^CART", ___)) { -->
<!--   # CART predictions -->
<!--   world$prob_DBF <- predict(___, newdata = ___, type = "___")[, "___"] -->

<!-- } else if (grepl("^Random Forest", ___)) { -->
<!--   # Random Forest predictions -->
<!--   world$prob_DBF <- predict(___, newdata = ___[, ___, drop = FALSE],  -->
<!--                             type = "___")[, "___"] -->

<!-- } else { -->
<!--   # GBM predictions -->
<!--   world$prob_DBF <- predict(___, newdata = ___, n.trees = ___,  -->
<!--                             type = "___") -->
<!-- } -->

<!-- # ============================================================================ -->
<!-- # STEP 4: CREATE GLOBAL MAP VISUALIZATION -->
<!-- # ============================================================================ -->

<!-- # Generate world map -->
<!-- # HINT: ggplot() with aes() sets aesthetics, geom_raster() creates grid -->
<!-- ggplot(___, aes(x = ___, y = ___, fill = ___)) + -->
<!--   geom_raster(interpolate = ___) +      # FALSE -->
<!--   coord_fixed() + -->
<!--   scale_fill_viridis_c(name = "___") +  # "P(TBMF)" -->
<!--   labs( -->
<!--     title = paste("Predicted probability of Deciduous Broadleaf Forests –",  -->
<!--                   ___),                   # best_name -->
<!--     x = "___",                           # "Longitude" -->
<!--     y = "___"                            # "Latitude" -->
<!--   ) -->
<!-- ``` -->

<!-- **Reflection Questions:** -->

<!-- 1. What risks arise when predicting outside the training climate envelope (extrapolation)? -->
<!-- 2. How should `P(TBMF)` be thresholded to produce a binary presence/absence map? -->
<!-- 3. Which uncertainty maps would be valuable additions to this analysis? -->

<!-- --- -->

